{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f44f15e",
   "metadata": {},
   "source": [
    "## Project 3 - Part 1\n",
    "\n",
    "Author: Qichun Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411e6a8",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "a) Create a new worksheet and name it Project3YourName.<br>\n",
    "b) Read in The Psy file, and display the tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8fddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Youtube01-Psy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4688ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>z13th1q4yzihf1bll23qxzpjeujterydj</td>\n",
       "      <td>Carmen Racasanu</td>\n",
       "      <td>2014-11-14T13:27:52</td>\n",
       "      <td>How can this have 2 billion views when there's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k</td>\n",
       "      <td>diego mogrovejo</td>\n",
       "      <td>2014-11-14T13:28:08</td>\n",
       "      <td>I don't now why I'm watching this in 2014﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>z130zd5b3titudkoe04ccbeohojxuzppvbg</td>\n",
       "      <td>BlueYetiPlayz -Call Of Duty and More</td>\n",
       "      <td>2015-05-23T13:04:32</td>\n",
       "      <td>subscribe to me for call of duty vids and give...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>z12he50arvrkivl5u04cctawgxzkjfsjcc4</td>\n",
       "      <td>Photo Editor</td>\n",
       "      <td>2015-06-05T14:14:48</td>\n",
       "      <td>hi guys please my android photo editor downloa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k</td>\n",
       "      <td>Ray Benich</td>\n",
       "      <td>2015-06-05T18:05:16</td>\n",
       "      <td>The first billion viewed this because they tho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                COMMENT_ID  \\\n",
       "345      z13th1q4yzihf1bll23qxzpjeujterydj   \n",
       "346  z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k   \n",
       "347    z130zd5b3titudkoe04ccbeohojxuzppvbg   \n",
       "348    z12he50arvrkivl5u04cctawgxzkjfsjcc4   \n",
       "349  z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k   \n",
       "\n",
       "                                   AUTHOR                 DATE  \\\n",
       "345                       Carmen Racasanu  2014-11-14T13:27:52   \n",
       "346                       diego mogrovejo  2014-11-14T13:28:08   \n",
       "347  BlueYetiPlayz -Call Of Duty and More  2015-05-23T13:04:32   \n",
       "348                          Photo Editor  2015-06-05T14:14:48   \n",
       "349                            Ray Benich  2015-06-05T18:05:16   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "345  How can this have 2 billion views when there's...      0  \n",
       "346         I don't now why I'm watching this in 2014﻿      0  \n",
       "347  subscribe to me for call of duty vids and give...      1  \n",
       "348  hi guys please my android photo editor downloa...      1  \n",
       "349  The first billion viewed this because they tho...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e4d5d",
   "metadata": {},
   "source": [
    "c) Give the number of rows in the files; and using query give the number of spam and not spam comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b100c80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3a7727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    175\n",
       "0    175\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba63155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 350 rows in the files. There are 175 spam comments and 175 not spam comments\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {df.shape[0]} rows in the files. There are {df['CLASS'].value_counts()[1]} spam comments and {df['CLASS'].value_counts()[0]} not spam comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61a440",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1cbfbb",
   "metadata": {},
   "source": [
    "a) Import the Bag of Words function and create an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0965427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb73bd4",
   "metadata": {},
   "source": [
    "b) Fit and transform the comments in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5693804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvec = vectorizer.fit_transform(df['CONTENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0843eccc",
   "metadata": {},
   "source": [
    "c) Output the fitted and transformed comments. You should have a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f9e6904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<350x1418 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4354 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44adb76f",
   "metadata": {},
   "source": [
    "d) What are the dimensions of the matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "010493c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 1418)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeaa362",
   "metadata": {},
   "source": [
    "The dimensions of the matrix is 350 x 1418. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c31dd",
   "metadata": {},
   "source": [
    "e) Print out the 349th comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13afe7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first billion viewed this because they thought it was really cool, the  other billion and a half came to see how stupid the first billion were...﻿\n"
     ]
    }
   ],
   "source": [
    "print(df['CONTENT'][349])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7032149",
   "metadata": {},
   "source": [
    "f) Using vectorizer.build_analyzer() display the breakdown of the 349th comment in to a “bag of words”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b998ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'first',\n",
       " 'billion',\n",
       " 'viewed',\n",
       " 'this',\n",
       " 'because',\n",
       " 'they',\n",
       " 'thought',\n",
       " 'it',\n",
       " 'was',\n",
       " 'really',\n",
       " 'cool',\n",
       " 'the',\n",
       " 'other',\n",
       " 'billion',\n",
       " 'and',\n",
       " 'half',\n",
       " 'came',\n",
       " 'to',\n",
       " 'see',\n",
       " 'how',\n",
       " 'stupid',\n",
       " 'the',\n",
       " 'first',\n",
       " 'billion',\n",
       " 'were']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(df['CONTENT'][349])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7d25d",
   "metadata": {},
   "source": [
    "g) Give the output of vectorizer.get_features_names(). What does this represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdfe9d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '02',\n",
       " '034',\n",
       " '05',\n",
       " '08',\n",
       " '10',\n",
       " '100',\n",
       " '100000415527985',\n",
       " '10200253113705769',\n",
       " '1030',\n",
       " '1073741828',\n",
       " '11',\n",
       " '1111',\n",
       " '112720997191206369631',\n",
       " '12',\n",
       " '123',\n",
       " '124',\n",
       " '124923004',\n",
       " '126',\n",
       " '127',\n",
       " '13017194',\n",
       " '131338190916',\n",
       " '1340488',\n",
       " '1340489',\n",
       " '1340490',\n",
       " '1340491',\n",
       " '1340492',\n",
       " '1340493',\n",
       " '1340494',\n",
       " '1340499',\n",
       " '1340500',\n",
       " '1340502',\n",
       " '1340503',\n",
       " '1340504',\n",
       " '1340517',\n",
       " '1340518',\n",
       " '1340519',\n",
       " '1340520',\n",
       " '1340521',\n",
       " '1340522',\n",
       " '1340523',\n",
       " '1340524',\n",
       " '134470083389909',\n",
       " '1415297812',\n",
       " '1495323920744243',\n",
       " '1496241863981208',\n",
       " '1496273723978022',\n",
       " '1498561870415874',\n",
       " '161620527267482',\n",
       " '171183229277',\n",
       " '19',\n",
       " '19924',\n",
       " '1firo',\n",
       " '1m',\n",
       " '20',\n",
       " '2009',\n",
       " '2012',\n",
       " '2012bitches',\n",
       " '2013',\n",
       " '2014',\n",
       " '201470069872822',\n",
       " '2015',\n",
       " '2017',\n",
       " '210',\n",
       " '23',\n",
       " '24',\n",
       " '24398',\n",
       " '243a',\n",
       " '279',\n",
       " '29',\n",
       " '2b',\n",
       " '2billion',\n",
       " '2x10',\n",
       " '300',\n",
       " '3000',\n",
       " '313327',\n",
       " '315',\n",
       " '322',\n",
       " '33',\n",
       " '33gxrf',\n",
       " '39',\n",
       " '390875584405933',\n",
       " '391725794320912',\n",
       " '40beuutvu2zkxk4utgpz8k',\n",
       " '4436607',\n",
       " '4604617',\n",
       " '48051',\n",
       " '484',\n",
       " '492',\n",
       " '4shared',\n",
       " '4snjqp',\n",
       " '4th',\n",
       " '50',\n",
       " '521',\n",
       " '5242575',\n",
       " '5277478',\n",
       " '5287',\n",
       " '57',\n",
       " '58',\n",
       " '5800',\n",
       " '5af506e1',\n",
       " '5c2f',\n",
       " '5million',\n",
       " '5s',\n",
       " '616375350',\n",
       " '636',\n",
       " '6381501',\n",
       " '694',\n",
       " '700',\n",
       " '733634264',\n",
       " '733949243353321',\n",
       " '734237113324534',\n",
       " '74',\n",
       " '750',\n",
       " '783',\n",
       " '79',\n",
       " '821',\n",
       " '884',\n",
       " '898',\n",
       " '8bit',\n",
       " '9082175',\n",
       " '9107',\n",
       " '9277547',\n",
       " '950',\n",
       " '969',\n",
       " '9bzkp7q19f0',\n",
       " '_0f9fa8aa',\n",
       " '__killuminati94',\n",
       " '_bzszz',\n",
       " '_chris_cz',\n",
       " '_fphgk5zllsvdqv0zuf0mb',\n",
       " '_gibu',\n",
       " '_o3h',\n",
       " '_ry6f57sprnd2xv',\n",
       " '_thqbeum69aqup1ih',\n",
       " '_trksid',\n",
       " '_vlczzrg8vgctlpsd9ongewhj8',\n",
       " 'aaaaaaa',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absolutely',\n",
       " 'access',\n",
       " 'accessories',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'acn2g',\n",
       " 'acting',\n",
       " 'active',\n",
       " 'actor',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'adding',\n",
       " 'admit',\n",
       " 'adsense',\n",
       " 'advice',\n",
       " 'affiliateid',\n",
       " 'after',\n",
       " 'again',\n",
       " 'ago',\n",
       " 'ahhh',\n",
       " 'al',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'allways',\n",
       " 'almond',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'alot',\n",
       " 'also',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'america',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'and',\n",
       " 'andrijamatf',\n",
       " 'android',\n",
       " 'angel',\n",
       " 'angry',\n",
       " 'animations',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyway',\n",
       " 'apocalypse',\n",
       " 'app',\n",
       " 'apparel',\n",
       " 'apparently',\n",
       " 'appreciate',\n",
       " 'apps',\n",
       " 'are',\n",
       " 'around',\n",
       " 'art',\n",
       " 'as',\n",
       " 'aseris',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'aspx',\n",
       " 'ass',\n",
       " 'at',\n",
       " 'attention',\n",
       " 'auburn',\n",
       " 'audiojungle',\n",
       " 'auditioning',\n",
       " 'avaaz',\n",
       " 'avoid',\n",
       " 'aw',\n",
       " 'away',\n",
       " 'aways',\n",
       " 'awesome',\n",
       " 'awesomeness',\n",
       " 'b00ecvf93g',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'band',\n",
       " 'bass',\n",
       " 'bd3721315',\n",
       " 'be',\n",
       " 'beautiful',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'beibs',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'between',\n",
       " 'bf4',\n",
       " 'bieber',\n",
       " 'big',\n",
       " 'bighit',\n",
       " 'bilion',\n",
       " 'billion',\n",
       " 'billions',\n",
       " 'billon',\n",
       " 'binbox',\n",
       " 'bing',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'blanc',\n",
       " 'block',\n",
       " 'blue',\n",
       " 'bomb',\n",
       " 'book',\n",
       " 'bother',\n",
       " 'bots',\n",
       " 'bottom',\n",
       " 'bowl',\n",
       " 'boyfriend',\n",
       " 'boys',\n",
       " 'bps',\n",
       " 'br',\n",
       " 'brain',\n",
       " 'brand',\n",
       " 'brew',\n",
       " 'bringing',\n",
       " 'brother',\n",
       " 'brotherhood',\n",
       " 'brothers',\n",
       " 'brt0u5',\n",
       " 'bs',\n",
       " 'bubblews',\n",
       " 'bucket',\n",
       " 'burned',\n",
       " 'but',\n",
       " 'butalabs',\n",
       " 'buy',\n",
       " 'by',\n",
       " 'c349',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'can',\n",
       " 'cant',\n",
       " 'capitalized',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'caroline',\n",
       " 'cash',\n",
       " 'cats',\n",
       " 'cd92db3f4',\n",
       " 'censor',\n",
       " 'certain',\n",
       " 'chacking',\n",
       " 'chainise',\n",
       " 'challenges',\n",
       " 'chance',\n",
       " 'chanel',\n",
       " 'change',\n",
       " 'chanicka',\n",
       " 'channel',\n",
       " 'channels',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'chhanel',\n",
       " 'chick',\n",
       " 'child',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'ching',\n",
       " 'chiptunes',\n",
       " 'christmas',\n",
       " 'chubbz',\n",
       " 'chuck',\n",
       " 'cirus',\n",
       " 'citizen',\n",
       " 'cjfoftxeba',\n",
       " 'cleaning',\n",
       " 'click',\n",
       " 'clicked',\n",
       " 'clip',\n",
       " 'close',\n",
       " 'clothes',\n",
       " 'clothing',\n",
       " 'co',\n",
       " 'code',\n",
       " 'codes',\n",
       " 'codytolleson',\n",
       " 'college',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comentars',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'comment_id',\n",
       " 'commenting',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'company',\n",
       " 'complaining',\n",
       " 'completely',\n",
       " 'concerts',\n",
       " 'condition',\n",
       " 'confidence',\n",
       " 'confirmed',\n",
       " 'connected',\n",
       " 'constructive',\n",
       " 'content',\n",
       " 'cool',\n",
       " 'could',\n",
       " 'count',\n",
       " 'countries',\n",
       " 'cover',\n",
       " 'covers',\n",
       " 'craft',\n",
       " 'crap',\n",
       " 'crazy',\n",
       " 'crdits',\n",
       " 'crea',\n",
       " 'credit',\n",
       " 'crew',\n",
       " 'criticism',\n",
       " 'crop',\n",
       " 'cross',\n",
       " 'crowd',\n",
       " 'cs',\n",
       " 'csgo',\n",
       " 'curled',\n",
       " 'cute',\n",
       " 'cvhmklt',\n",
       " 'cxpzpgb',\n",
       " 'czfcxsn0jnq',\n",
       " 'da',\n",
       " 'daaaaaaaaaaannng',\n",
       " 'dad',\n",
       " 'dafuq',\n",
       " 'daily',\n",
       " 'dance',\n",
       " 'dances',\n",
       " 'dancing',\n",
       " 'day',\n",
       " 'dealing',\n",
       " 'dear',\n",
       " 'december',\n",
       " 'decent',\n",
       " 'dedicated',\n",
       " 'defuse',\n",
       " 'deserve',\n",
       " 'designs',\n",
       " 'details',\n",
       " 'dick',\n",
       " 'did',\n",
       " 'diddle',\n",
       " 'didn',\n",
       " 'die',\n",
       " 'difference',\n",
       " 'difficult',\n",
       " 'dinero',\n",
       " 'ding',\n",
       " 'direction',\n",
       " 'disappointed',\n",
       " 'discover',\n",
       " 'dislike',\n",
       " 'dislikes',\n",
       " 'dislikesssssssssssssssssssssssssssssssss',\n",
       " 'divertenti',\n",
       " 'diys',\n",
       " 'dizzy',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " 'doing',\n",
       " 'dolacz',\n",
       " 'dominate',\n",
       " 'don',\n",
       " 'donate',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'download',\n",
       " 'downloading',\n",
       " 'dresses',\n",
       " 'dressprettyonce',\n",
       " 'driving',\n",
       " 'drone',\n",
       " 'drones',\n",
       " 'drop',\n",
       " 'drugs',\n",
       " 'drunk',\n",
       " 'dubstep',\n",
       " 'dumb',\n",
       " 'dunno',\n",
       " 'during',\n",
       " 'duty',\n",
       " 'dylan',\n",
       " 'earn',\n",
       " 'earth',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'easypromosapp',\n",
       " 'ebay',\n",
       " 'ede05ea397ca',\n",
       " 'editor',\n",
       " 'edm',\n",
       " 'eeccon',\n",
       " 'effects',\n",
       " 'effort',\n",
       " 'ehi',\n",
       " 'ejw9kvkoxdamqm808h5z',\n",
       " 'elevator',\n",
       " 'eliminate',\n",
       " 'emerson_zanol',\n",
       " 'end',\n",
       " 'english',\n",
       " 'enimen',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'enter',\n",
       " 'entertaining',\n",
       " 'entire',\n",
       " 'epic',\n",
       " 'equals',\n",
       " 'ermail',\n",
       " 'esteem',\n",
       " 'etc',\n",
       " 'eugenekalinin',\n",
       " 'eve',\n",
       " 'even',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyday',\n",
       " 'everyone',\n",
       " 'everyones',\n",
       " 'everything',\n",
       " 'ex',\n",
       " 'excuse',\n",
       " 'expectations',\n",
       " 'expensive',\n",
       " 'experiments',\n",
       " 'explore',\n",
       " 'exposure',\n",
       " 'eyebrows',\n",
       " 'eyes',\n",
       " 'f7',\n",
       " 'fablife',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'facts',\n",
       " 'fail',\n",
       " 'fake',\n",
       " 'family',\n",
       " 'fanboys',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fashionable',\n",
       " 'fb',\n",
       " 'featured',\n",
       " 'feedback',\n",
       " 'feel',\n",
       " 'festival',\n",
       " 'few',\n",
       " 'fighting',\n",
       " 'figure',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'fireball',\n",
       " 'first',\n",
       " 'fish',\n",
       " 'flipagram',\n",
       " 'flute',\n",
       " 'fly',\n",
       " 'follow',\n",
       " 'follower',\n",
       " 'football',\n",
       " 'for',\n",
       " 'forget',\n",
       " 'foto',\n",
       " 'found',\n",
       " 'four',\n",
       " 'freddy',\n",
       " 'free',\n",
       " 'freemyapps',\n",
       " 'french',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'frigea',\n",
       " 'from',\n",
       " 'fruity',\n",
       " 'fuck',\n",
       " 'fucked',\n",
       " 'fucken',\n",
       " 'fucking',\n",
       " 'fudairyqueen',\n",
       " 'fuego',\n",
       " 'fun',\n",
       " 'funnier',\n",
       " 'funny',\n",
       " 'funnytortspics',\n",
       " 'future',\n",
       " 'gabriel',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gamestop',\n",
       " 'gaming',\n",
       " 'ganga',\n",
       " 'gangam',\n",
       " 'gangman',\n",
       " 'gangnam',\n",
       " 'gangnamstyle',\n",
       " 'gatti',\n",
       " 'gay',\n",
       " 'gbphotographygb',\n",
       " 'gcmforex',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'ghost',\n",
       " 'gift',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'giveaways',\n",
       " 'giver',\n",
       " 'glasses',\n",
       " 'go',\n",
       " 'goal',\n",
       " 'goes',\n",
       " 'gofundme',\n",
       " 'going',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'goodbye',\n",
       " 'google',\n",
       " 'gook',\n",
       " 'got',\n",
       " 'gotta',\n",
       " 'gotten',\n",
       " 'government',\n",
       " 'gp',\n",
       " 'grateful',\n",
       " 'great',\n",
       " 'greetings',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'grwmps',\n",
       " 'gt',\n",
       " 'gta5',\n",
       " 'guardalo',\n",
       " 'gun',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'gvr7xg',\n",
       " 'gwar',\n",
       " 'hacked',\n",
       " 'hackers',\n",
       " 'hackfbaccountlive',\n",
       " 'had',\n",
       " 'haha',\n",
       " 'hahah',\n",
       " 'hahahahah',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'halftime',\n",
       " 'halp',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'hassle',\n",
       " 'hate',\n",
       " 'haters',\n",
       " 'hav',\n",
       " 'have',\n",
       " 'having',\n",
       " 'he',\n",
       " 'head',\n",
       " 'headbutt',\n",
       " 'hear',\n",
       " 'heart',\n",
       " 'heck',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'him',\n",
       " 'hip',\n",
       " 'his',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'hits',\n",
       " 'hl',\n",
       " 'hole',\n",
       " 'holy',\n",
       " 'hop',\n",
       " 'hope',\n",
       " 'hoppa',\n",
       " 'hour',\n",
       " 'how',\n",
       " 'html',\n",
       " 'http',\n",
       " 'https',\n",
       " 'huge',\n",
       " 'huh',\n",
       " 'humanity',\n",
       " 'hunger',\n",
       " 'hw',\n",
       " 'hwang',\n",
       " 'hyperurl',\n",
       " 'hyuna',\n",
       " 'ice',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ig',\n",
       " 'il',\n",
       " 'ill',\n",
       " 'illuminati',\n",
       " 'im',\n",
       " 'image2you',\n",
       " 'imagine',\n",
       " 'improve',\n",
       " 'in',\n",
       " 'including',\n",
       " 'incmedia',\n",
       " 'indiegogo',\n",
       " 'inspired',\n",
       " 'instagram',\n",
       " 'instagraml',\n",
       " 'internet',\n",
       " 'into',\n",
       " 'inviting',\n",
       " 'io',\n",
       " 'iphone',\n",
       " 'iq2',\n",
       " 'irl',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'isnt',\n",
       " 'it',\n",
       " 'ithat',\n",
       " 'itm',\n",
       " 'its',\n",
       " 'itunes',\n",
       " 'itz',\n",
       " 'jackal',\n",
       " 'jackson',\n",
       " 'jae',\n",
       " 'james',\n",
       " 'jap',\n",
       " 'jaroadc',\n",
       " 'jb',\n",
       " 'jelly',\n",
       " 'jellyfish',\n",
       " 'jenny',\n",
       " 'job',\n",
       " 'join',\n",
       " 'joint2',\n",
       " 'joke',\n",
       " 'joking',\n",
       " 'jr',\n",
       " 'jtcjnmho',\n",
       " 'juice',\n",
       " 'julie',\n",
       " 'julien',\n",
       " 'juno',\n",
       " 'just',\n",
       " 'justin',\n",
       " 'juyk',\n",
       " 'jyp',\n",
       " 'k6a5xt',\n",
       " 'kamal',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'keyword',\n",
       " 'kickstarter',\n",
       " 'kids',\n",
       " 'kidsmediausa',\n",
       " 'kidz',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'king',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'kobyoshi02',\n",
       " 'kodysman',\n",
       " 'koean',\n",
       " 'kollektivet',\n",
       " 'korea',\n",
       " 'korean',\n",
       " 'koreans',\n",
       " 'kyle',\n",
       " 'l2649',\n",
       " 'l551h',\n",
       " 'la',\n",
       " 'lacked',\n",
       " 'lada',\n",
       " 'language',\n",
       " 'last',\n",
       " 'later',\n",
       " 'laugh',\n",
       " 'launchpad',\n",
       " 'lazy',\n",
       " 'leader',\n",
       " 'league',\n",
       " 'leah',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'lexis',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likes',\n",
       " 'liking',\n",
       " 'limit',\n",
       " 'ling',\n",
       " 'link',\n",
       " 'linkbucks',\n",
       " 'listen',\n",
       " 'listening',\n",
       " 'listing',\n",
       " 'lists',\n",
       " 'little',\n",
       " 'littlebrother',\n",
       " 'live',\n",
       " 'll',\n",
       " 'lnuj',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lool',\n",
       " 'loool',\n",
       " 'loops',\n",
       " 'lordviperas',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'loving',\n",
       " 'low',\n",
       " 'lt',\n",
       " 'lucas',\n",
       " 'lucks',\n",
       " 'luka1qmrhf',\n",
       " 'luther',\n",
       " 'lyrics',\n",
       " 'm1555',\n",
       " 'mabey',\n",
       " 'made',\n",
       " 'make',\n",
       " 'making',\n",
       " 'man',\n",
       " 'management',\n",
       " 'many',\n",
       " 'mario',\n",
       " 'marius',\n",
       " 'market',\n",
       " 'marketglory',\n",
       " 'markusmairhofer',\n",
       " 'martin',\n",
       " 'mathster',\n",
       " 'may',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'mee',\n",
       " 'meets',\n",
       " 'members',\n",
       " 'memories',\n",
       " 'men',\n",
       " 'mercury',\n",
       " 'meselx',\n",
       " 'michael',\n",
       " 'miley',\n",
       " 'milions',\n",
       " 'million',\n",
       " 'millions',\n",
       " 'millioon',\n",
       " 'millisecond',\n",
       " 'millon',\n",
       " 'min',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'minecraft',\n",
       " 'minoo',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'mix',\n",
       " 'model',\n",
       " 'mom',\n",
       " 'mon',\n",
       " 'money',\n",
       " 'monkey',\n",
       " 'monkeys',\n",
       " 'montages',\n",
       " 'month',\n",
       " 'months',\n",
       " 'more',\n",
       " 'morgage',\n",
       " 'moroccan',\n",
       " 'most',\n",
       " 'mother',\n",
       " 'moved',\n",
       " 'movie',\n",
       " 'mp3s',\n",
       " 'ms',\n",
       " 'mscalifornia95',\n",
       " 'msmarilynmiles',\n",
       " 'much',\n",
       " 'multiple',\n",
       " 'murdev',\n",
       " 'music',\n",
       " 'must',\n",
       " 'mute',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'nail',\n",
       " 'nails',\n",
       " 'name',\n",
       " 'national',\n",
       " 'necks',\n",
       " 'need',\n",
       " 'neon',\n",
       " 'net',\n",
       " 'network',\n",
       " 'never',\n",
       " 'new',\n",
       " 'newest',\n",
       " 'news',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'nicushorbboy',\n",
       " 'night',\n",
       " 'nike',\n",
       " 'ninja',\n",
       " 'no',\n",
       " 'non',\n",
       " 'norrus',\n",
       " 'not',\n",
       " 'notch',\n",
       " 'now',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'obsessed',\n",
       " 'odowd',\n",
       " 'of',\n",
       " 'off',\n",
       " 'offer',\n",
       " 'officialpsy',\n",
       " 'offset',\n",
       " 'offıcal',\n",
       " 'often',\n",
       " 'old',\n",
       " 'older',\n",
       " 'olds',\n",
       " 'oldspice',\n",
       " 'olp_tab_refurbished',\n",
       " 'omg',\n",
       " 'on',\n",
       " 'once',\n",
       " 'oncueapparel',\n",
       " 'one',\n",
       " 'only',\n",
       " 'oppa',\n",
       " 'or',\n",
       " 'org',\n",
       " 'other',\n",
       " 'our',\n",
       " 'out',\n",
       " 'outfit',\n",
       " 'ovbiously',\n",
       " 'over',\n",
       " 'own',\n",
       " 'p3984',\n",
       " 'page',\n",
       " 'pages',\n",
       " 'paid',\n",
       " 'pal',\n",
       " 'pan',\n",
       " 'pants',\n",
       " 'part',\n",
       " 'partners',\n",
       " 'party',\n",
       " 'passed',\n",
       " 'pause',\n",
       " 'pay',\n",
       " 'pazzi',\n",
       " 'pdf',\n",
       " 'pe',\n",
       " 'peace',\n",
       " 'penis',\n",
       " 'people',\n",
       " 'peoples',\n",
       " 'perfect',\n",
       " 'perform',\n",
       " 'permpage',\n",
       " 'person',\n",
       " 'petition',\n",
       " 'petitions',\n",
       " 'phenomena',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'php',\n",
       " 'pictures',\n",
       " 'piece',\n",
       " 'pivot',\n",
       " 'pl',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'planet',\n",
       " 'platform',\n",
       " 'play',\n",
       " 'please',\n",
       " 'plizz',\n",
       " 'pls',\n",
       " 'plus',\n",
       " 'plz',\n",
       " 'pnref',\n",
       " 'po',\n",
       " 'point',\n",
       " 'pop',\n",
       " 'popaegis',\n",
       " 'popular',\n",
       " 'population',\n",
       " 'populatoin',\n",
       " 'portfolio',\n",
       " 'post',\n",
       " 'posting',\n",
       " 'posts',\n",
       " 'pouring',\n",
       " 'pplease',\n",
       " 'pray',\n",
       " 'praying',\n",
       " 'prehistoric',\n",
       " 'premium',\n",
       " 'pretty',\n",
       " 'preview',\n",
       " 'pride',\n",
       " 'problem',\n",
       " 'prod',\n",
       " 'producer',\n",
       " 'project',\n",
       " 'projects',\n",
       " 'promise',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988ee7f",
   "metadata": {},
   "source": [
    "Each one is a tokenized word. The function returns a list of feature names selected from the raw documents. We can identify words that were found after vectorizing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0e1dc",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "a) Shuffle the dataset (frac=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ded5e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de226d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>z13neh044nq2wn0o404citkh2taufzqxuz40k</td>\n",
       "      <td>Mason Sieverding</td>\n",
       "      <td>2014-11-07T01:50:10</td>\n",
       "      <td>Please check out my vidios﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>z13gsp34uumcdje4q04civk54zr4wr0gxjc</td>\n",
       "      <td>Lucky D.</td>\n",
       "      <td>2014-11-02T14:53:06</td>\n",
       "      <td>http://hackfbaccountlive.com/?ref=4604617﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>z13mebgjooulgzdkt23kzvlqjyrzx32n0</td>\n",
       "      <td>amine moha</td>\n",
       "      <td>2014-11-06T18:11:37</td>\n",
       "      <td>please like : http://www.bubblews.com/news/927...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>z12zstfhixudundzf04cjbsp0rjrzh1rw3k</td>\n",
       "      <td>MiningBip3</td>\n",
       "      <td>2014-11-06T02:22:00</td>\n",
       "      <td>Check out pivot animations in my channel﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>z124i3ygclj0xn4ly22jzb5a2ubvtbev1</td>\n",
       "      <td>Abdinasir Omar</td>\n",
       "      <td>2014-11-05T15:28:28</td>\n",
       "      <td>Great music anyway﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                COMMENT_ID            AUTHOR  \\\n",
       "181  z13neh044nq2wn0o404citkh2taufzqxuz40k  Mason Sieverding   \n",
       "76     z13gsp34uumcdje4q04civk54zr4wr0gxjc          Lucky D.   \n",
       "166      z13mebgjooulgzdkt23kzvlqjyrzx32n0        amine moha   \n",
       "147    z12zstfhixudundzf04cjbsp0rjrzh1rw3k        MiningBip3   \n",
       "125      z124i3ygclj0xn4ly22jzb5a2ubvtbev1    Abdinasir Omar   \n",
       "\n",
       "                    DATE                                            CONTENT  \\\n",
       "181  2014-11-07T01:50:10                        Please check out my vidios﻿   \n",
       "76   2014-11-02T14:53:06         http://hackfbaccountlive.com/?ref=4604617﻿   \n",
       "166  2014-11-06T18:11:37  please like : http://www.bubblews.com/news/927...   \n",
       "147  2014-11-06T02:22:00          Check out pivot animations in my channel﻿   \n",
       "125  2014-11-05T15:28:28                                Great music anyway﻿   \n",
       "\n",
       "     CLASS  \n",
       "181      1  \n",
       "76       1  \n",
       "166      1  \n",
       "147      1  \n",
       "125      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30693bc",
   "metadata": {},
   "source": [
    "b) Create your training and testing sets. Use the first 300 entries for training and the remaining for testing. Name the files d_train, and d_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a2ac835",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = df[:300]\n",
    "d_test = df[300:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ff381",
   "metadata": {},
   "source": [
    "c) Create your training and testing attributes BOW using using vectorizer.fit_transform. Name these d_train_att, and d_test_att."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d04805d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_att = vectorizer.fit_transform(d_train['CONTENT'])\n",
    "d_test_att = vectorizer.transform(d_test['CONTENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e693f71",
   "metadata": {},
   "source": [
    "d) Create your training and testing labels. Name these d_train_label, and d_test_label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "016a5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_label = d_train['CLASS']\n",
    "d_test_label = d_test['CLASS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96404ea3",
   "metadata": {},
   "source": [
    "e) Output both d_train_att, and d_test_att. What are the dimensions of the matrices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b18e7a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<300x1288 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3753 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train_att"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e602cf",
   "metadata": {},
   "source": [
    "The dimensions of the d_train_att matrice is 300x1288."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "134d32b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50x1288 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 470 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test_att"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb5e2d",
   "metadata": {},
   "source": [
    "The dimensions of the d_test_att matrice is 50x1288. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1004eb82",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "a) Crate a random forest classifier with 80 trees. Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb95f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "befb840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(d_train_att, d_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b45dd",
   "metadata": {},
   "source": [
    "b) Using .score, score your tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd10ccd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(d_test_att, d_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb86014",
   "metadata": {},
   "source": [
    "c) Create a confusion matrix for the test labels and prediction labels, and out put his array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60c60e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predicted_labels = clf.predict(d_test_att)\n",
    "cm = confusion_matrix(d_test_label, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "857cbd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28,  0],\n",
       "       [ 1, 21]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64c4fc",
   "metadata": {},
   "source": [
    "d) Cross validate and output your accuracy, i.e., scores.mean +/- 2sd. Make sure to use your training attributes and training labels here.Set cv=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6b8dfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_RF = cross_val_score(clf, d_train_att, d_train_label, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b802a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Random Forest: 0.97 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the Random Forest: %0.2f (+/- %0.2f)\"%(scores_RF.mean(), scores_RF.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669a464",
   "metadata": {},
   "source": [
    "e) In your words explain the cross validation step, and what it is used for. You may use any references you wish. Please include any references you use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50b00d",
   "metadata": {},
   "source": [
    "The first step of cross-validation is to split the data into subsets(or folds). Then, it will treat each subset as a training set or a validating set. For example, when we set the cv=5, the dataset will be divided into 5 folds. Then, it will experiment five times where the first time treats the first fold as a validating set and the remaining four folds as training. Each experiment will split the data as 80% training and 20% validation. We repeat the experiment five times until each fold has been a validation set one time. \n",
    "\n",
    "||fold 1| fold 2 | fold 3 |fold 4|fold 5|\n",
    "|:-| :-: | :-: | :-: | :-: | :-: |\n",
    "|Experiment 1 | Validation | Training | Training | Training | Training |\n",
    "|Experiment 2 | Training | Validation| Training | Training | Training |\n",
    "|Experiment 3 | Training | Training | Validation| Training | Training |\n",
    "|Experiment 4 | Training | Training | Training | Validation | Training |\n",
    "|Experiment 5 | Training | Training | Training | Training | Validation|\n",
    "\n",
    "The cross-validation is used for validating the machine learning model to ensure its accuracy and measure its performance. The reason to validate the machine learning model is to make sure the model is able to perform well on unseen data. The cross-validation usually provides a more accurate measurement of the performance of the model because it allows the model to train and test on different splits. \n",
    "\n",
    "__References:__<br>\n",
    "3.1. Cross-validation: evaluating estimator performance. (n.d.). Scikit-learn. Retrieved October 25, 2022, from https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446bae7",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "a) Load and concatenate all 5 comments files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c7aedab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(map(pd.read_csv, ['Youtube01-Psy.csv', \n",
    "                                   'Youtube02-KatyPerry.csv', \n",
    "                                   'Youtube03-LMFAO.csv',\n",
    "                                   'Youtube04-Eminem.csv',\n",
    "                                   'Youtube05-Shakira.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae2c14fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0e5e4",
   "metadata": {},
   "source": [
    "b) Display the length of the file as well as the breakdown into spam and non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15c826d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1956"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of the file\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b41515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of the spam\n",
    "len(data[data['CLASS']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7bb8961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of the non-spam\n",
    "len(data[data['CLASS']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8918e146",
   "metadata": {},
   "source": [
    "c) Shuffle the new data, and create content and label sets, d_content, and d_label respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e007d2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>z13shj4wpmflidcxc04ce5f4vlqdyzjowso0k</td>\n",
       "      <td>Tornike Noniashvili</td>\n",
       "      <td>2014-11-08T04:08:09</td>\n",
       "      <td>subscribe my chanel﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>z13lhdo5vnyndj3p304cjjs4zze3sldrqbg</td>\n",
       "      <td>Mai Nguyễn</td>\n",
       "      <td>2014-08-28T03:53:35</td>\n",
       "      <td>help me click on the subscribe Mai Nguyen, tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>z132jbmxfqm4fjysg23nwjfb2mv2vxnua</td>\n",
       "      <td>Decio Alves Martins</td>\n",
       "      <td>2015-06-05T19:29:20</td>\n",
       "      <td>KATY PERRY, I AM THE \"DÉCIO CABELO\", \"DECIO HA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>z12mdpxzzvmxwfevl23juhzibvqyvp52s</td>\n",
       "      <td>Martwy Karas</td>\n",
       "      <td>2014-11-07T19:33:46</td>\n",
       "      <td>Katy Perry's songs are the best of the songs o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>z12xizopmzbch3dje04cibrzbqi2x54ptgk</td>\n",
       "      <td>Shaina Rodriguez</td>\n",
       "      <td>2015-05-16T00:20:41.387000</td>\n",
       "      <td>lol so funny love it﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                COMMENT_ID               AUTHOR  \\\n",
       "261  z13shj4wpmflidcxc04ce5f4vlqdyzjowso0k  Tornike Noniashvili   \n",
       "40     z13lhdo5vnyndj3p304cjjs4zze3sldrqbg           Mai Nguyễn   \n",
       "347      z132jbmxfqm4fjysg23nwjfb2mv2vxnua  Decio Alves Martins   \n",
       "313      z12mdpxzzvmxwfevl23juhzibvqyvp52s         Martwy Karas   \n",
       "203    z12xizopmzbch3dje04cibrzbqi2x54ptgk     Shaina Rodriguez   \n",
       "\n",
       "                           DATE  \\\n",
       "261         2014-11-08T04:08:09   \n",
       "40          2014-08-28T03:53:35   \n",
       "347         2015-06-05T19:29:20   \n",
       "313         2014-11-07T19:33:46   \n",
       "203  2015-05-16T00:20:41.387000   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "261                               subscribe my chanel﻿      1  \n",
       "40   help me click on the subscribe Mai Nguyen, tha...      1  \n",
       "347  KATY PERRY, I AM THE \"DÉCIO CABELO\", \"DECIO HA...      1  \n",
       "313  Katy Perry's songs are the best of the songs o...      0  \n",
       "203                              lol so funny love it﻿      0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d75feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_content = data['CONTENT']\n",
    "d_label = data['CLASS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6e318",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b1ba98",
   "metadata": {},
   "source": [
    "a) Read the scikit learn documentation on Pipelines. What is an advantage to using a pipeline? Use any reference you wish to answer the question, just be sure to include the reference in your response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e0a9a",
   "metadata": {},
   "source": [
    "Pipeline allows to perform cross-validation while setting different parameters for the model. The advantages to use a pipeline are:\n",
    "<ul>\n",
    "    <li>Machine learning workflow will be easy to read and understand</li>\n",
    "    <li>Enforce the implementation and order of steps in your model</li>\n",
    "    <li>Easy to reproduce</li>\n",
    "</ul>\n",
    "\n",
    "__References:__<br>\n",
    "sklearn.pipeline.Pipeline. (n.d.). Scikit-learn. Retrieved October 25, 2022, from https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html<br>\n",
    "\n",
    "Vickery, R. (2021, December 7). A Simple Guide to Scikit-learn Pipelines - vickdata. Medium. https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e81481",
   "metadata": {},
   "source": [
    "b) Import both Pipeline and make_pipeline from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d643827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957092a1",
   "metadata": {},
   "source": [
    "c) Create a pipeline. You may use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ba1adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bag-of-word', CountVectorizer()),\n",
    "    ('random forest', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe1ae0",
   "metadata": {},
   "source": [
    "d) Output the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1843954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bag-of-word',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabul...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eeaa5ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('countvectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, voc...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or: pipeline = make_pipeline(CountVectorizer(), RandomForestClassifier())\n",
    "make_pipeline(CountVectorizer(), RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814d0b3",
   "metadata": {},
   "source": [
    "e) Fit your pipeline with the first 1500 entries of the content and the labels. Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3084cc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bag-of-word',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabul...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(d_content[:1500], d_label[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ccc9df",
   "metadata": {},
   "source": [
    "f) Use .score to score your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbc2e3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(d_content[1500:], d_label[1500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29d624",
   "metadata": {},
   "source": [
    "g) Use your pipeline to predict whether the following two comments are spam or Not spam. <br>\n",
    "I. “what a neat video!”<br>\n",
    "II. “plz subscribe to my channel”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d37742a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"what a neat video!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8daaf79",
   "metadata": {},
   "source": [
    "It returns 0, so it is not a spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52da6504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"plz subscribe to my channel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f9c20",
   "metadata": {},
   "source": [
    "It returns 1, so it is a spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38526203",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed6b385",
   "metadata": {},
   "source": [
    "a) Cross validate your pipeline using d_content and d_label. Set cv=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db919c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pipeline = cross_val_score(pipeline, d_content, d_label, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82acc9f9",
   "metadata": {},
   "source": [
    "b) Print out the accuracy(scores.mean) +/- 2sd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "568fc92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the pipeline: 0.96 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the pipeline: %0.2f (+/- %0.2f)\"%(scores_pipeline.mean(), scores_pipeline.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2bd31f",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "a) Create a second pipeline named pipeline2 which incorporates the TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af398820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add tfidf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "pipeline2 = make_pipeline(CountVectorizer(),\n",
    "                          TfidfTransformer(norm=None),\n",
    "                          RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92416859",
   "metadata": {},
   "source": [
    "b) Cross validate pipeline2 using d_content and d_labels.Output the accuracy(scores.mean) +/- 2sd. Set cv=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10b58cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pipeline2 = cross_val_score(pipeline2, d_content, d_label, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c68402dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the pipeline2: 0.96 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the pipeline2: %0.2f (+/- %0.2f)\"%(scores_pipeline2.mean(), scores_pipeline2.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa9dcf8",
   "metadata": {},
   "source": [
    "c) Output the steps of pipeline 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33a0df10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countvectorizer',\n",
       "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                  dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                  lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                  ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                  strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                  tokenizer=None, vocabulary=None)),\n",
       " ('tfidftransformer',\n",
       "  TfidfTransformer(norm=None, smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       " ('randomforestclassifier',\n",
       "  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                         criterion='gini', max_depth=None, max_features='auto',\n",
       "                         max_leaf_nodes=None, max_samples=None,\n",
       "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                         min_samples_leaf=1, min_samples_split=2,\n",
       "                         min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                         n_jobs=None, oob_score=False, random_state=None,\n",
       "                         verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0d95a",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "a) Use the following code snippet to set up a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22d91ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter search\n",
    "parameters = {\n",
    "    'countvectorizer__max_features': (None, 1000, 2000),\n",
    "    'countvectorizer__ngram_range': ((1,1), (1, 2)), # unigrams or bigrams\n",
    "    'countvectorizer__stop_words': ('english', None),\n",
    "    'tfidftransformer__use_idf': (True, False), # effectively turn on/off tfidf\n",
    "    'randomforestclassifier__n_estimators': (20, 50, 100)\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline2, parameters, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b53d8b",
   "metadata": {},
   "source": [
    "b) Describe the purpose of the code in part (a) within the context of the grid search. What is this setting up?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1552c5",
   "metadata": {},
   "source": [
    "The grid search allows the user to specify different hyperparameters and values and calculates the performance for each combination and find the optimal performance of those hyperparameters settings. The first thing within the context is the estimator, which in our case is pipeline2, it is the model that we want to apply. The second thing within the context of the grid search is the param_grid, which is the parameter that we defined to store all of the values of the hyperparameters with parameter names as the key. The n_jobs is the number of jobs that we want to run in parallel, -1 means using all processors. The last one verbose controls the verbosity, a higher value indicates more messages will be displayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76edd21",
   "metadata": {},
   "source": [
    "c) Using .fit nd d_content, and d_labels, perform the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5fa50f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   13.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('countvectorizer',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token...\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'countvectorizer__max_features': (None, 1000, 2000),\n",
       "                         'countvectorizer__ngram_range': ((1, 1), (1, 2)),\n",
       "                         'countvectorizer__stop_words': ('english', None),\n",
       "                         'randomforestclassifier__n_estimators': (20, 50, 100),\n",
       "                         'tfidftransformer__use_idf': (True, False)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(d_content, d_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9358cb6",
   "metadata": {},
   "source": [
    "d) Print out the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed8dd1",
   "metadata": {},
   "source": [
    "i. Best score<br>\n",
    "ii. Best countvectorizer_max_features value<br>\n",
    "iii. Best countvectorizer_ngram_range value<br>\n",
    "iv. Stop words setting<br>\n",
    "v. Best n_estimators setting value<br>\n",
    "vi. TF-IDF setting<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b141dbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('countvectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=1000, min_df=1,\n",
       "                                 ngram_range=(1, 2), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, voc...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "102cdac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__max_features': 1000,\n",
       " 'countvectorizer__ngram_range': (1, 2),\n",
       " 'countvectorizer__stop_words': None,\n",
       " 'randomforestclassifier__n_estimators': 100,\n",
       " 'tfidftransformer__use_idf': False}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af1ac138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.962\n",
      "Best parameters set:\n",
      "\tcountvectorizer__max_features: 1000\n",
      "\tcountvectorizer__ngram_range: (1, 2)\n",
      "\tcountvectorizer__stop_words: None\n",
      "\trandomforestclassifier__n_estimators: 100\n",
      "\ttfidftransformer__use_idf: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: {0:.3f}\".format(grid_search.best_score_))\n",
    "print(\"Best parameters set:\")\n",
    "for parameter, value in grid_search.best_params_.items():\n",
    "    print(f\"\\t{parameter}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe05cce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013caad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
