{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "001fc231",
   "metadata": {},
   "source": [
    "## Project 3 - Part 2\n",
    "\n",
    "Author: Qichun Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d228866",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "a) Import gensim and logging . Using the scikit documentation give a short description of both gensim and logging. You may use the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "527f1548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacks\\anaconda3\\envs\\UL\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed026ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s :%(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0bb5a6",
   "metadata": {},
   "source": [
    "b) Load the Google w2v model. You may have to locate this file on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70356d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 00:26:36,100 :INFO : loading projection weights from GoogleNews-vectors-negative300.bin\n",
      "2022-10-30 00:26:59,026 :INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from GoogleNews-vectors-negative300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2022-10-30T00:26:59.025831', 'gensim': '4.0.1', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "gmodel = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa948c",
   "metadata": {},
   "source": [
    "c) Display the vector for the following words.\n",
    "<ol>\n",
    "    <li>‘cat’</li>\n",
    "    <li>‘dog’</li>\n",
    "    <li>‘spatula’</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "761359ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
       "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656,\n",
       "        0.08056641, -0.5859375 , -0.00445557, -0.296875  , -0.01312256,\n",
       "       -0.08349609,  0.05053711,  0.15136719, -0.44921875, -0.0135498 ,\n",
       "        0.21484375, -0.14746094,  0.22460938, -0.125     , -0.09716797,\n",
       "        0.24902344, -0.2890625 ,  0.36523438,  0.41210938, -0.0859375 ,\n",
       "       -0.07861328, -0.19726562, -0.09082031, -0.14160156, -0.10253906,\n",
       "        0.13085938, -0.00346375,  0.07226562,  0.04418945,  0.34570312,\n",
       "        0.07470703, -0.11230469,  0.06738281,  0.11230469,  0.01977539,\n",
       "       -0.12353516,  0.20996094, -0.07226562, -0.02783203,  0.05541992,\n",
       "       -0.33398438,  0.08544922,  0.34375   ,  0.13964844,  0.04931641,\n",
       "       -0.13476562,  0.16308594, -0.37304688,  0.39648438,  0.10693359,\n",
       "        0.22167969,  0.21289062, -0.08984375,  0.20703125,  0.08935547,\n",
       "       -0.08251953,  0.05957031,  0.10205078, -0.19238281, -0.09082031,\n",
       "        0.4921875 ,  0.03955078, -0.07080078, -0.0019989 , -0.23046875,\n",
       "        0.25585938,  0.08984375, -0.10644531,  0.00105286, -0.05883789,\n",
       "        0.05102539, -0.0291748 ,  0.19335938, -0.14160156, -0.33398438,\n",
       "        0.08154297, -0.27539062,  0.10058594, -0.10449219, -0.12353516,\n",
       "       -0.140625  ,  0.03491211, -0.11767578, -0.1796875 , -0.21484375,\n",
       "       -0.23828125,  0.08447266, -0.07519531, -0.25976562, -0.21289062,\n",
       "       -0.22363281, -0.09716797,  0.11572266,  0.15429688,  0.07373047,\n",
       "       -0.27539062,  0.14257812, -0.0201416 ,  0.10009766, -0.19042969,\n",
       "       -0.09375   ,  0.14160156,  0.17089844,  0.3125    , -0.16699219,\n",
       "       -0.08691406, -0.05004883, -0.24902344, -0.20800781, -0.09423828,\n",
       "       -0.12255859, -0.09472656, -0.390625  , -0.06640625, -0.31640625,\n",
       "        0.10986328, -0.00156403,  0.04345703,  0.15625   , -0.18945312,\n",
       "       -0.03491211,  0.03393555, -0.14453125,  0.01611328, -0.14160156,\n",
       "       -0.02392578,  0.01501465,  0.07568359,  0.10742188,  0.12695312,\n",
       "        0.10693359, -0.01184082, -0.24023438,  0.0291748 ,  0.16210938,\n",
       "        0.19921875, -0.28125   ,  0.16699219, -0.11621094, -0.25585938,\n",
       "        0.38671875, -0.06640625, -0.4609375 , -0.06176758, -0.14453125,\n",
       "       -0.11621094,  0.05688477,  0.03588867, -0.10693359,  0.18847656,\n",
       "       -0.16699219, -0.01794434,  0.10986328, -0.12353516, -0.16308594,\n",
       "       -0.14453125,  0.12890625,  0.11523438,  0.13671875,  0.05688477,\n",
       "       -0.08105469, -0.06152344, -0.06689453,  0.27929688, -0.19628906,\n",
       "        0.07226562,  0.12304688, -0.20996094, -0.22070312,  0.21386719,\n",
       "       -0.1484375 , -0.05932617,  0.05224609,  0.06445312, -0.02636719,\n",
       "        0.13183594,  0.19433594,  0.27148438,  0.18652344,  0.140625  ,\n",
       "        0.06542969, -0.14453125,  0.05029297,  0.08837891,  0.12255859,\n",
       "        0.26757812,  0.0534668 , -0.32226562, -0.20703125,  0.18164062,\n",
       "        0.04418945, -0.22167969, -0.13769531, -0.04174805, -0.00286865,\n",
       "        0.04077148,  0.07275391, -0.08300781,  0.08398438, -0.3359375 ,\n",
       "       -0.40039062,  0.01757812, -0.18652344, -0.0480957 , -0.19140625,\n",
       "        0.10107422,  0.09277344, -0.30664062, -0.19921875, -0.0168457 ,\n",
       "        0.12207031,  0.14648438, -0.12890625, -0.23535156, -0.05371094,\n",
       "       -0.06640625,  0.06884766, -0.03637695,  0.2109375 , -0.06005859,\n",
       "        0.19335938,  0.05151367, -0.05322266,  0.02893066, -0.27539062,\n",
       "        0.08447266,  0.328125  ,  0.01818848,  0.01495361,  0.04711914,\n",
       "        0.37695312, -0.21875   , -0.03393555,  0.01116943,  0.36914062,\n",
       "        0.02160645,  0.03466797,  0.07275391,  0.16015625, -0.16503906,\n",
       "       -0.296875  ,  0.15039062, -0.29101562,  0.13964844,  0.00448608,\n",
       "        0.171875  , -0.21972656,  0.09326172, -0.19042969,  0.01599121,\n",
       "       -0.09228516,  0.15722656, -0.14160156, -0.0534668 ,  0.03613281,\n",
       "        0.23632812, -0.15136719, -0.00689697, -0.27148438, -0.07128906,\n",
       "       -0.16503906,  0.18457031, -0.08398438,  0.18554688,  0.11669922,\n",
       "        0.02758789, -0.04760742,  0.17871094,  0.06542969, -0.03540039,\n",
       "        0.22949219,  0.02697754, -0.09765625,  0.26953125,  0.08349609,\n",
       "       -0.13085938, -0.10107422, -0.00738525,  0.07128906,  0.14941406,\n",
       "       -0.20605469,  0.18066406, -0.15820312,  0.05932617,  0.28710938,\n",
       "       -0.04663086,  0.15136719,  0.4921875 , -0.27539062,  0.05615234],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Cat\n",
    "gmodel['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b448ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01,\n",
       "       -8.44726562e-02,  5.73730469e-02,  5.85937500e-02, -8.25195312e-02,\n",
       "       -1.53808594e-02, -6.34765625e-02,  1.79687500e-01, -4.23828125e-01,\n",
       "       -2.25830078e-02, -1.66015625e-01, -2.51464844e-02,  1.07421875e-01,\n",
       "       -1.99218750e-01,  1.59179688e-01, -1.87500000e-01, -1.20117188e-01,\n",
       "        1.55273438e-01, -9.91210938e-02,  1.42578125e-01, -1.64062500e-01,\n",
       "       -8.93554688e-02,  2.00195312e-01, -1.49414062e-01,  3.20312500e-01,\n",
       "        3.28125000e-01,  2.44140625e-02, -9.71679688e-02, -8.20312500e-02,\n",
       "       -3.63769531e-02, -8.59375000e-02, -9.86328125e-02,  7.78198242e-03,\n",
       "       -1.34277344e-02,  5.27343750e-02,  1.48437500e-01,  3.33984375e-01,\n",
       "        1.66015625e-02, -2.12890625e-01, -1.50756836e-02,  5.24902344e-02,\n",
       "       -1.07421875e-01, -8.88671875e-02,  2.49023438e-01, -7.03125000e-02,\n",
       "       -1.59912109e-02,  7.56835938e-02, -7.03125000e-02,  1.19140625e-01,\n",
       "        2.29492188e-01,  1.41601562e-02,  1.15234375e-01,  7.50732422e-03,\n",
       "        2.75390625e-01, -2.44140625e-01,  2.96875000e-01,  3.49121094e-02,\n",
       "        2.42187500e-01,  1.35742188e-01,  1.42578125e-01,  1.75781250e-02,\n",
       "        2.92968750e-02, -1.21582031e-01,  2.28271484e-02, -4.76074219e-02,\n",
       "       -1.55273438e-01,  3.14331055e-03,  3.45703125e-01,  1.22558594e-01,\n",
       "       -1.95312500e-01,  8.10546875e-02, -6.83593750e-02, -1.47094727e-02,\n",
       "        2.14843750e-01, -1.21093750e-01,  1.57226562e-01, -2.07031250e-01,\n",
       "        1.36718750e-01, -1.29882812e-01,  5.29785156e-02, -2.71484375e-01,\n",
       "       -2.98828125e-01, -1.84570312e-01, -2.29492188e-01,  1.19140625e-01,\n",
       "        1.53198242e-02, -2.61718750e-01, -1.23046875e-01, -1.86767578e-02,\n",
       "       -6.49414062e-02, -8.15429688e-02,  7.86132812e-02, -3.53515625e-01,\n",
       "        5.24902344e-02, -2.45361328e-02, -5.43212891e-03, -2.08984375e-01,\n",
       "       -2.10937500e-01, -1.79687500e-01,  2.42187500e-01,  2.57812500e-01,\n",
       "        1.37695312e-01, -2.10937500e-01, -2.17285156e-02, -1.38671875e-01,\n",
       "        1.84326172e-02, -1.23901367e-02, -1.59179688e-01,  1.61132812e-01,\n",
       "        2.08007812e-01,  1.03027344e-01,  9.81445312e-02, -6.83593750e-02,\n",
       "       -8.72802734e-03, -2.89062500e-01, -2.14843750e-01, -1.14257812e-01,\n",
       "       -2.21679688e-01,  4.12597656e-02, -3.12500000e-01, -5.59082031e-02,\n",
       "       -9.76562500e-02,  5.81054688e-02, -4.05273438e-02, -1.73828125e-01,\n",
       "        1.64062500e-01, -2.53906250e-01, -1.54296875e-01, -2.31933594e-02,\n",
       "       -2.38281250e-01,  2.07519531e-02, -2.73437500e-01,  3.90625000e-03,\n",
       "        1.13769531e-01, -1.73828125e-01,  2.57812500e-01,  2.35351562e-01,\n",
       "        5.22460938e-02,  6.83593750e-02, -1.75781250e-01,  1.60156250e-01,\n",
       "       -5.98907471e-04,  5.98144531e-02, -2.11914062e-01, -5.54199219e-02,\n",
       "       -7.51953125e-02, -3.06640625e-01,  4.27734375e-01,  5.32226562e-02,\n",
       "       -2.08984375e-01, -5.71289062e-02, -2.09960938e-01,  3.29589844e-02,\n",
       "        1.05468750e-01, -1.50390625e-01, -9.37500000e-02,  1.16699219e-01,\n",
       "        6.44531250e-02,  2.80761719e-02,  2.41210938e-01, -1.25976562e-01,\n",
       "       -1.00585938e-01, -1.22680664e-02, -3.26156616e-04,  1.58691406e-02,\n",
       "        1.27929688e-01, -3.32031250e-02,  4.07714844e-02, -1.31835938e-01,\n",
       "        9.81445312e-02,  1.74804688e-01, -2.36328125e-01,  5.17578125e-02,\n",
       "        1.83593750e-01,  2.42919922e-02, -4.31640625e-01,  2.46093750e-01,\n",
       "       -3.03955078e-02, -2.47802734e-02, -1.17187500e-01,  1.61132812e-01,\n",
       "       -5.71289062e-02,  1.16577148e-02,  2.81250000e-01,  4.27734375e-01,\n",
       "        4.56542969e-02,  1.01074219e-01, -3.95507812e-02,  1.77001953e-02,\n",
       "       -8.98437500e-02,  1.35742188e-01,  2.08007812e-01,  1.88476562e-01,\n",
       "       -1.52343750e-01, -2.37304688e-01, -1.90429688e-01,  7.12890625e-02,\n",
       "       -2.46093750e-01, -2.61718750e-01, -2.34375000e-01, -1.45507812e-01,\n",
       "       -1.17187500e-02, -1.50390625e-01, -1.13281250e-01,  1.82617188e-01,\n",
       "        2.63671875e-01, -1.37695312e-01, -4.58984375e-01, -4.68750000e-02,\n",
       "       -1.26953125e-01, -4.22363281e-02, -1.66992188e-01,  1.26953125e-01,\n",
       "        2.59765625e-01, -2.44140625e-01, -2.19726562e-01, -8.69140625e-02,\n",
       "        1.59179688e-01, -3.78417969e-02,  8.97216797e-03, -2.77343750e-01,\n",
       "       -1.04980469e-01, -1.75781250e-01,  2.28515625e-01, -2.70996094e-02,\n",
       "        2.85156250e-01, -2.73437500e-01,  1.61132812e-02,  5.90820312e-02,\n",
       "       -2.39257812e-01,  1.77734375e-01, -1.34765625e-01,  1.38671875e-01,\n",
       "        3.53515625e-01,  1.22070312e-01,  1.43554688e-01,  9.22851562e-02,\n",
       "        2.29492188e-01, -3.00781250e-01, -4.88281250e-02, -1.79687500e-01,\n",
       "        2.96875000e-01,  1.75781250e-01,  4.80957031e-02, -3.38745117e-03,\n",
       "        7.91015625e-02, -2.38281250e-01, -2.31445312e-01,  1.66015625e-01,\n",
       "       -2.13867188e-01, -7.03125000e-02, -7.56835938e-02,  1.96289062e-01,\n",
       "       -1.29882812e-01, -1.05957031e-01, -3.53515625e-01, -1.16699219e-01,\n",
       "       -5.10253906e-02,  3.39355469e-02, -1.43554688e-01, -3.90625000e-03,\n",
       "        1.73828125e-01, -9.96093750e-02, -1.66015625e-01, -8.54492188e-02,\n",
       "       -3.82812500e-01,  5.90820312e-02, -6.22558594e-02,  8.83789062e-02,\n",
       "       -8.88671875e-02,  3.28125000e-01,  6.83593750e-02, -1.91406250e-01,\n",
       "       -8.35418701e-04,  1.04003906e-01,  1.52343750e-01, -1.53350830e-03,\n",
       "        4.16015625e-01, -3.32031250e-02,  1.49414062e-01,  2.42187500e-01,\n",
       "       -1.76757812e-01, -4.93164062e-02, -1.24511719e-01,  1.25976562e-01,\n",
       "        1.74804688e-01,  2.81250000e-01, -1.80664062e-01,  1.03027344e-01,\n",
       "       -2.75390625e-01,  2.61718750e-01,  2.46093750e-01, -4.71191406e-02,\n",
       "        6.25000000e-02,  4.16015625e-01, -3.55468750e-01,  2.22656250e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Dog\n",
    "gmodel['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d7d764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19140625, -0.04296875,  0.27539062,  0.00488281, -0.3203125 ,\n",
       "        0.08203125,  0.05566406, -0.03613281, -0.31445312,  0.10693359,\n",
       "       -0.359375  ,  0.29882812,  0.02331543,  0.05517578, -0.140625  ,\n",
       "        0.1953125 , -0.23632812, -0.22167969, -0.06542969, -0.3359375 ,\n",
       "        0.25195312, -0.09326172,  0.54296875,  0.11328125, -0.28710938,\n",
       "       -0.12011719, -0.11181641,  0.20996094, -0.33203125,  0.30273438,\n",
       "       -0.3359375 , -0.12255859,  0.12890625, -0.28515625, -0.04223633,\n",
       "        0.25585938,  0.3203125 ,  0.07177734,  0.19042969, -0.01379395,\n",
       "        0.16992188, -0.22460938,  0.5078125 ,  0.08398438, -0.07519531,\n",
       "       -0.06396484,  0.05371094,  0.34570312,  0.46289062, -0.16699219,\n",
       "       -0.30664062,  0.15234375, -0.09765625, -0.26171875, -0.14160156,\n",
       "        0.2265625 ,  0.49609375, -0.10791016, -0.08447266,  0.234375  ,\n",
       "        0.04931641, -0.07128906,  0.05273438, -0.11914062,  0.09814453,\n",
       "        0.11181641, -0.13574219, -0.46875   ,  0.26171875,  0.12158203,\n",
       "        0.31445312,  0.05810547,  0.0703125 , -0.10107422, -0.27734375,\n",
       "       -0.16796875, -0.07128906, -0.08007812,  0.07226562, -0.1484375 ,\n",
       "        0.22949219,  0.03686523, -0.03857422,  0.00616455, -0.12255859,\n",
       "       -0.01940918, -0.0625    ,  0.26953125,  0.34179688, -0.00427246,\n",
       "       -0.49023438, -0.38867188, -0.24316406,  0.12304688,  0.07421875,\n",
       "       -0.25195312,  0.14941406, -0.265625  ,  0.30859375, -0.05834961,\n",
       "       -0.19726562, -0.14941406,  0.01031494, -0.07275391, -0.23632812,\n",
       "       -0.1484375 ,  0.3046875 , -0.10351562,  0.69140625, -0.29492188,\n",
       "       -0.25976562, -0.29296875,  0.31445312, -0.11083984, -0.5       ,\n",
       "       -0.04443359,  0.10058594,  0.04858398, -0.0625    ,  0.02001953,\n",
       "        0.08837891, -0.10058594, -0.00113678,  0.390625  ,  0.16894531,\n",
       "        0.01318359,  0.15625   ,  0.10595703,  0.29296875,  0.18457031,\n",
       "        0.13867188,  0.15820312, -0.38671875,  0.20214844,  0.17285156,\n",
       "        0.578125  , -0.05029297, -0.34179688,  0.46875   ,  0.34765625,\n",
       "        0.23535156, -0.20996094, -0.32226562, -0.26367188, -0.02160645,\n",
       "        0.50390625, -0.31054688, -0.2265625 ,  0.31640625, -0.14550781,\n",
       "        0.05639648, -0.18847656,  0.05126953,  0.73828125,  0.04174805,\n",
       "       -0.02392578, -0.375     , -0.18847656, -0.0625    ,  0.50390625,\n",
       "       -0.02844238, -0.05615234,  0.26367188, -0.0612793 ,  0.06030273,\n",
       "       -0.171875  , -0.00866699,  0.20019531, -0.03173828, -0.18164062,\n",
       "        0.40429688, -0.03710938, -0.03515625,  0.18261719, -0.47070312,\n",
       "       -0.07275391,  0.32617188, -0.17285156, -0.22265625,  0.27929688,\n",
       "       -0.57421875,  0.07275391,  0.20898438, -0.30273438,  0.19335938,\n",
       "        0.04907227, -0.15820312, -0.17675781,  0.18066406,  0.42773438,\n",
       "       -0.25390625,  0.29101562, -0.04760742, -0.28710938, -0.08837891,\n",
       "        0.28710938, -0.3828125 , -0.13574219,  0.05297852, -0.22265625,\n",
       "       -0.09179688, -0.06738281, -0.53125   ,  0.06933594,  0.02514648,\n",
       "        0.04443359, -0.18457031, -0.31054688,  0.02856445,  0.16992188,\n",
       "        0.01196289, -0.12109375,  0.00430298,  0.171875  ,  0.06640625,\n",
       "       -0.02954102, -0.39453125,  0.515625  ,  0.2109375 ,  0.03637695,\n",
       "       -0.390625  , -0.04980469, -0.13378906, -0.19140625, -0.34375   ,\n",
       "       -0.21289062,  0.375     ,  0.03039551,  0.1796875 , -0.37109375,\n",
       "        0.07763672, -0.07666016,  0.07910156, -0.15234375, -0.15429688,\n",
       "       -0.15039062,  0.12304688,  0.1796875 , -0.19335938,  0.125     ,\n",
       "        0.17285156,  0.03173828,  0.24121094, -0.34765625, -0.15136719,\n",
       "       -0.09619141,  0.09326172,  0.13964844, -0.20410156, -0.3671875 ,\n",
       "        0.02368164, -0.24804688,  0.17578125,  0.22460938,  0.18652344,\n",
       "       -0.44921875, -0.1640625 , -0.08837891,  0.1484375 , -0.0480957 ,\n",
       "        0.23144531, -0.06176758, -0.53125   , -0.23730469, -0.26953125,\n",
       "        0.02770996, -0.09667969, -0.05249023, -0.00543213, -0.05981445,\n",
       "        0.3203125 ,  0.34570312, -0.06591797, -0.05786133,  0.54296875,\n",
       "       -0.17675781, -0.05810547, -0.0378418 , -0.14550781, -0.265625  ,\n",
       "       -0.01153564,  0.3515625 ,  0.2421875 ,  0.24023438,  0.01025391,\n",
       "       -0.31445312, -0.20410156, -0.02172852, -0.21386719,  0.52734375,\n",
       "        0.12451172,  0.16113281, -0.46289062,  0.05126953, -0.02648926,\n",
       "       -0.546875  ,  0.30859375, -0.0534668 , -0.34765625,  0.3984375 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Spatula\n",
    "gmodel['spatula']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d866d2",
   "metadata": {},
   "source": [
    "d) Output the similarity scores for the following pairs.\n",
    "<ol>\n",
    "    <li>‘dog’ , ‘cat’</li>\n",
    "    <li>‘dog’ , ‘spatula’</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb32db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76094574"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity scores for 'dog' and 'cat'\n",
    "gmodel.similarity('dog','cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb553ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17059729"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity scores for 'dog' and 'spatula'\n",
    "gmodel.similarity('dog', 'spatula')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814554c",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147cb25",
   "metadata": {},
   "source": [
    "a) Use the following code to train your model. We will use doc2vec as we wan a vector for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "203779d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae439d",
   "metadata": {},
   "source": [
    "b) Using the scikit documentation give a brief description of the genism library, and the TaggedDocument class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce1b09",
   "metadata": {},
   "source": [
    "Genism library is free open source Python library for natural language processing(NLP). The library includes topic modelling, document indexing and similarity retrieval with large corpora. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0edaf6d",
   "metadata": {},
   "source": [
    "c) Incorporate the following utility function into your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74999550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'<[^>]+>', ' ', sent) \n",
    "    sent = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', sent) \n",
    "    sent = re.sub(r'\\W', ' ', sent) \n",
    "    sent = re.sub(r'\\s+', ' ', sent)\n",
    "    sent = sent.strip()\n",
    "    return sent.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b4964",
   "metadata": {},
   "source": [
    "d) Describe exactly what the utility function is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4fc32",
   "metadata": {},
   "source": [
    "The utility takes sent as input. In the first line of code change the words to lowercase. The second to the fifth line are using the re.sub(), which is a function in the built-in re module to handle regular expressions. The codes remove any HTML tags, apostrophes, punctuation, and repeated spaces from the sent. After that, the function uses the strip() method to remove any spaces at the beginning and at the end of the string. In the end, it returns and uses the split() method to spit a string into a list so that each word will be separated and stored individually into a list. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d7c73",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8670b612",
   "metadata": {},
   "source": [
    "a) Use the code snippet in your worksheet. You will need to access the files in the comments of the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be5ed2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "164720\n",
      "175325\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "unsup_sentences = []\n",
    "\n",
    "for dirname in [\"train/pos\", \"train/neg\", \"train/unsup\", \"test/pos\", \"test/neg\"]:\n",
    "    for fname in sorted(os.listdir(\"aclimdb/\" + dirname)):\n",
    "        if fname[-4:] == \".txt\":\n",
    "            with open(\"aclimdb/\" + dirname + \"/\" + fname, encoding='UTF-8') as f:\n",
    "                sent = f.read()\n",
    "                words = extract_words(sent)\n",
    "                unsup_sentences.append(TaggedDocument(words,[dirname + \"/\" + fname]))\n",
    "print(len(unsup_sentences))\n",
    "\n",
    "for dirname in [\"review_polarity/txt_sentoken/pos\",\"review_polarity/txt_sentoken/neg\"]:\n",
    "    for fname in sorted(os.listdir(dirname)):\n",
    "        if fname[-4:] == \".txt\":\n",
    "             with open(dirname + \"/\" + fname, encoding='UTF-8') as f:\n",
    "                    for i, sent in enumerate(f):\n",
    "                        words = extract_words(sent)\n",
    "                        unsup_sentences.append(TaggedDocument(words,[\"%s/%s-%d\" % (dirname, fname,i)]))\n",
    "print(len(unsup_sentences))\n",
    "\n",
    "with open(\"stanfordSentimentTreebank/original_rt_snippets.txt\", encoding = 'UTF-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        words = extract_words(line)\n",
    "        unsup_sentences.append(TaggedDocument(words,[\"rt-%d\" %i]))\n",
    "\n",
    "print(len(unsup_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8fb82",
   "metadata": {},
   "source": [
    "b) Explain the function of the code in part(a). Explain each loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9648a5",
   "metadata": {},
   "source": [
    "We first create an empty list called unsup_sentences. The first for loop opens all the txt files within the aclImdb folder, which contains IMDB review data. The aclImdb dataset includes 50,000 labelled reviews (25,000 for the training dataset and 25,000 for the testing dataset) and 50,000 unlabelled movie reviews. As we can see the len of the unsup_sentences list is 100,000. The TaggedDocument helps to represent a document along with a tag. We pass the words from the txt files as the input document with the tag that has the information of dirname(indicates whether the text is from training or testing) and fname(name of the file). \n",
    "\n",
    "The second for loop opens all the txt files within the review_polarity folder, which also contains movie review data but the datasets come from cornell.edu. The dataset includes 67,720 review data and is split into neg(negative) and pos(positive). The for loop is doing similar things to the first one to use TaggedDocument to store text with a tag. \n",
    "\n",
    "The third for loop is a simple loop that reads the data file oriiginal_rt_snippets.txt from Rotten Tomatoes. The files contain 10,605 processed snippets from the original pool of Rotten Tomatoes. The loop also uses TaggedDocument to store reviews with a tag for further training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab48e16",
   "metadata": {},
   "source": [
    "c) After loading all of this how many training examples are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "532dfad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 175325 training examples are available in the unsup_sentences list.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(unsup_sentences)} training examples are available in the unsup_sentences list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa384e8",
   "metadata": {},
   "source": [
    "d) Display the first 10 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c61777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'hig', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', 'at', 'high', 'a', 'classic', 'line', 'inspector', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', 'student', 'welcome', 'to', 'bromwell', 'high', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', 'what', 'a', 'pity', 'that', 'it', 'is'], tags=['train/pos/0_9.txt']),\n",
       " TaggedDocument(words=['homelessness', 'or', 'houselessness', 'as', 'george', 'carlin', 'stated', 'has', 'been', 'an', 'issue', 'for', 'years', 'but', 'never', 'a', 'plan', 'to', 'help', 'those', 'on', 'the', 'street', 'that', 'were', 'once', 'considered', 'human', 'who', 'did', 'everything', 'from', 'going', 'to', 'school', 'work', 'or', 'vote', 'for', 'the', 'matter', 'most', 'people', 'think', 'of', 'the', 'homeless', 'as', 'just', 'a', 'lost', 'cause', 'while', 'worrying', 'about', 'things', 'such', 'as', 'racism', 'the', 'war', 'on', 'iraq', 'pressuring', 'kids', 'to', 'succeed', 'technology', 'the', 'elections', 'inflation', 'or', 'worrying', 'if', 'the', 'l', 'be', 'next', 'to', 'end', 'up', 'on', 'the', 'streets', 'but', 'what', 'if', 'you', 'were', 'given', 'a', 'bet', 'to', 'live', 'on', 'the', 'streets', 'for', 'a', 'month', 'without', 'the', 'luxuries', 'you', 'once', 'had', 'from', 'a', 'home', 'the', 'entertainment', 'sets', 'a', 'bathroom', 'pictures', 'on', 'the', 'wall', 'a', 'computer', 'and', 'everything', 'you', 'once', 'treasure', 'to', 'see', 'what', 'i', 'like', 'to', 'be', 'homeless', 'that', 'is', 'goddard', 'bol', 'lesson', 'mel', 'brooks', 'who', 'directs', 'who', 'stars', 'as', 'bolt', 'plays', 'a', 'rich', 'man', 'who', 'has', 'everything', 'in', 'the', 'world', 'until', 'deciding', 'to', 'make', 'a', 'bet', 'with', 'a', 'sissy', 'rival', 'jeffery', 'tambor', 'to', 'see', 'if', 'he', 'can', 'live', 'in', 'the', 'streets', 'for', 'thirty', 'days', 'without', 'the', 'luxuries', 'if', 'bolt', 'succeeds', 'he', 'can', 'do', 'what', 'he', 'wants', 'with', 'a', 'future', 'project', 'of', 'making', 'more', 'buildings', 'the', 'be', 'on', 'where', 'bolt', 'is', 'thrown', 'on', 'the', 'street', 'with', 'a', 'bracelet', 'on', 'his', 'leg', 'to', 'monitor', 'his', 'every', 'move', 'where', 'he', 'ca', 'step', 'off', 'the', 'sidewalk', 'h', 'given', 'the', 'nickname', 'pepto', 'by', 'a', 'vagrant', 'after', 'i', 'written', 'on', 'his', 'forehead', 'where', 'bolt', 'meets', 'other', 'characters', 'including', 'a', 'woman', 'by', 'the', 'name', 'of', 'molly', 'lesley', 'ann', 'warren', 'an', 'ex', 'dancer', 'who', 'got', 'divorce', 'before', 'losing', 'her', 'home', 'and', 'her', 'pals', 'sailor', 'howard', 'morris', 'and', 'fumes', 'teddy', 'wilson', 'who', 'are', 'already', 'used', 'to', 'the', 'streets', 'the', 'e', 'survivors', 'bolt', 'is', 'h', 'not', 'used', 'to', 'reaching', 'mutual', 'agreements', 'like', 'he', 'once', 'did', 'when', 'being', 'rich', 'where', 'i', 'fight', 'or', 'flight', 'kill', 'or', 'be', 'killed', 'while', 'the', 'love', 'connection', 'between', 'molly', 'and', 'bolt', 'was', 'necessary', 'to', 'plot', 'i', 'found', 'life', 'stinks', 'to', 'be', 'one', 'of', 'mel', 'brooks', 'observant', 'films', 'where', 'prior', 'to', 'being', 'a', 'comedy', 'it', 'shows', 'a', 'tender', 'side', 'compared', 'to', 'his', 'slapstick', 'work', 'such', 'as', 'blazing', 'saddles', 'young', 'frankenstein', 'or', 'spaceballs', 'for', 'the', 'matter', 'to', 'show', 'what', 'i', 'like', 'having', 'something', 'valuable', 'before', 'losing', 'it', 'the', 'next', 'day', 'or', 'on', 'the', 'other', 'hand', 'making', 'a', 'stupid', 'bet', 'like', 'all', 'rich', 'people', 'do', 'when', 'they', 'do', 'know', 'what', 'to', 'do', 'with', 'their', 'money', 'maybe', 'they', 'should', 'give', 'it', 'to', 'the', 'homeless', 'instead', 'of', 'using', 'it', 'like', 'monopoly', 'money', 'or', 'maybe', 'this', 'film', 'will', 'inspire', 'you', 'to', 'help', 'others'], tags=['train/pos/10000_8.txt']),\n",
       " TaggedDocument(words=['brilliant', 'over', 'acting', 'by', 'lesley', 'ann', 'warren', 'best', 'dramatic', 'hobo', 'lady', 'i', 'have', 'ever', 'seen', 'and', 'love', 'scenes', 'in', 'clothes', 'warehouse', 'are', 'second', 'to', 'none', 'the', 'corn', 'on', 'face', 'is', 'a', 'classic', 'as', 'good', 'as', 'anything', 'in', 'blazing', 'saddles', 'the', 'take', 'on', 'lawyers', 'is', 'also', 'superb', 'after', 'being', 'accused', 'of', 'being', 'a', 'turncoat', 'selling', 'out', 'his', 'boss', 'and', 'being', 'dishonest', 'the', 'lawyer', 'of', 'pepto', 'bolt', 'shrugs', 'indifferently', 'a', 'lawyer', 'he', 'says', 'three', 'funny', 'words', 'jeffrey', 'tambor', 'a', 'favorite', 'from', 'the', 'later', 'larry', 'sanders', 'show', 'is', 'fantastic', 'here', 'too', 'as', 'a', 'mad', 'millionaire', 'who', 'wants', 'to', 'crush', 'the', 'ghetto', 'his', 'character', 'is', 'more', 'malevolent', 'than', 'usual', 'the', 'hospital', 'scene', 'and', 'the', 'scene', 'where', 'the', 'homeless', 'invade', 'a', 'demolition', 'site', 'are', 'all', 'time', 'classics', 'look', 'for', 'the', 'legs', 'scene', 'and', 'the', 'two', 'big', 'diggers', 'fighting', 'one', 'bleeds', 'this', 'movie', 'gets', 'better', 'each', 'time', 'i', 'see', 'it', 'which', 'is', 'quite', 'often'], tags=['train/pos/10001_10.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'easily', 'the', 'most', 'underrated', 'film', 'inn', 'the', 'brooks', 'cannon', 'sure', 'its', 'flawed', 'it', 'does', 'not', 'give', 'a', 'realistic', 'view', 'of', 'homelessness', 'unlike', 'say', 'how', 'citizen', 'kane', 'gave', 'a', 'realistic', 'view', 'of', 'lounge', 'singers', 'or', 'titanic', 'gave', 'a', 'realistic', 'view', 'of', 'italians', 'you', 'idiots', 'many', 'of', 'the', 'jokes', 'fall', 'flat', 'but', 'still', 'this', 'film', 'is', 'very', 'lovable', 'in', 'a', 'way', 'many', 'comedies', 'are', 'not', 'and', 'to', 'pull', 'that', 'off', 'in', 'a', 'story', 'about', 'some', 'of', 'the', 'most', 'traditionally', 'reviled', 'members', 'of', 'society', 'is', 'truly', 'impressive', 'its', 'not', 'the', 'fisher', 'king', 'but', 'its', 'not', 'crap', 'either', 'my', 'only', 'complaint', 'is', 'that', 'brooks', 'should', 'have', 'cast', 'someone', 'else', 'in', 'the', 'lead', 'i', 'love', 'mel', 'as', 'a', 'director', 'and', 'writer', 'not', 'so', 'much', 'as', 'a', 'lead'], tags=['train/pos/10002_7.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'not', 'the', 'typical', 'mel', 'brooks', 'film', 'it', 'was', 'much', 'less', 'slapstick', 'than', 'most', 'of', 'his', 'movies', 'and', 'actually', 'had', 'a', 'plot', 'that', 'was', 'followable', 'leslie', 'ann', 'warren', 'made', 'the', 'movie', 'she', 'is', 'such', 'a', 'fantastic', 'under', 'rated', 'actress', 'there', 'were', 'some', 'moments', 'that', 'could', 'have', 'been', 'fleshed', 'out', 'a', 'bit', 'more', 'and', 'some', 'scenes', 'that', 'could', 'probably', 'have', 'been', 'cut', 'to', 'make', 'the', 'room', 'to', 'do', 'so', 'but', 'all', 'in', 'all', 'this', 'is', 'worth', 'the', 'price', 'to', 'rent', 'and', 'see', 'it', 'the', 'acting', 'was', 'good', 'overall', 'brooks', 'himself', 'did', 'a', 'good', 'job', 'without', 'his', 'characteristic', 'speaking', 'to', 'directly', 'to', 'the', 'audience', 'again', 'warren', 'was', 'the', 'best', 'actor', 'in', 'the', 'movie', 'but', 'fume', 'and', 'sailor', 'both', 'played', 'their', 'parts', 'well'], tags=['train/pos/10003_8.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'the', 'comedic', 'robin', 'williams', 'nor', 'is', 'it', 'the', 'quirky', 'insane', 'robin', 'williams', 'of', 'recent', 'thriller', 'fame', 'this', 'is', 'a', 'hybrid', 'of', 'the', 'classic', 'drama', 'without', 'over', 'dramatization', 'mixed', 'with', 'robi', 'new', 'love', 'of', 'the', 'thriller', 'but', 'this', 'is', 'a', 'thriller', 'per', 'se', 'this', 'is', 'more', 'a', 'mystery', 'suspense', 'vehicle', 'through', 'which', 'williams', 'attempts', 'to', 'locate', 'a', 'sick', 'boy', 'and', 'his', 'keeper', 'also', 'starring', 'sandra', 'oh', 'and', 'rory', 'culkin', 'this', 'suspense', 'drama', 'plays', 'pretty', 'much', 'like', 'a', 'news', 'report', 'until', 'willia', 'character', 'gets', 'close', 'to', 'achieving', 'his', 'goal', 'i', 'must', 'say', 'that', 'i', 'was', 'highly', 'entertained', 'though', 'this', 'movie', 'fails', 'to', 'teach', 'guide', 'inspect', 'or', 'amuse', 'it', 'felt', 'more', 'like', 'i', 'was', 'watching', 'a', 'guy', 'williams', 'as', 'he', 'was', 'actually', 'performing', 'the', 'actions', 'from', 'a', 'third', 'person', 'perspective', 'in', 'other', 'words', 'it', 'felt', 'real', 'and', 'i', 'was', 'able', 'to', 'subscribe', 'to', 'the', 'premise', 'of', 'the', 'story', 'all', 'in', 'all', 'i', 'worth', 'a', 'watch', 'though', 'i', 'definitely', 'not', 'friday', 'saturday', 'night', 'fare', 'it', 'rates', 'a', '7', '7', '10', 'from', 'the', 'fiend'], tags=['train/pos/10004_8.txt']),\n",
       " TaggedDocument(words=['yes', 'its', 'an', 'art', 'to', 'successfully', 'make', 'a', 'slow', 'paced', 'thriller', 'the', 'story', 'unfolds', 'in', 'nice', 'volumes', 'while', 'you', 'do', 'even', 'notice', 'it', 'happening', 'fine', 'performance', 'by', 'robin', 'williams', 'the', 'sexuality', 'angles', 'in', 'the', 'film', 'can', 'seem', 'unnecessary', 'and', 'can', 'probably', 'affect', 'how', 'much', 'you', 'enjoy', 'the', 'film', 'however', 'the', 'core', 'plot', 'is', 'very', 'engaging', 'the', 'movie', 'does', 'rush', 'onto', 'you', 'and', 'still', 'grips', 'you', 'enough', 'to', 'keep', 'you', 'wondering', 'the', 'direction', 'is', 'good', 'use', 'of', 'lights', 'to', 'achieve', 'desired', 'affects', 'of', 'suspense', 'and', 'unexpectedness', 'is', 'good', 'very', 'nice', '1', 'time', 'watch', 'if', 'you', 'are', 'looking', 'to', 'lay', 'back', 'and', 'hear', 'a', 'thrilling', 'short', 'story'], tags=['train/pos/10005_7.txt']),\n",
       " TaggedDocument(words=['in', 'this', 'critically', 'acclaimed', 'psychological', 'thriller', 'based', 'on', 'true', 'events', 'gabriel', 'robin', 'williams', 'a', 'celebrated', 'writer', 'and', 'late', 'night', 'talk', 'show', 'host', 'becomes', 'captivated', 'by', 'the', 'harrowing', 'story', 'of', 'a', 'young', 'listener', 'and', 'his', 'adoptive', 'mother', 'toni', 'collette', 'when', 'troubling', 'questions', 'arise', 'about', 'this', 'bo', 'story', 'however', 'gabriel', 'finds', 'himself', 'drawn', 'into', 'a', 'widening', 'mystery', 'that', 'hides', 'a', 'deadly', 'secret', 'according', 'to', 'fil', 'official', 'synopsis', 'you', 'really', 'should', 'stop', 'reading', 'these', 'comments', 'and', 'watch', 'the', 'film', 'now', 'the', 'how', 'did', 'he', 'lose', 'his', 'leg', 'ending', 'with', 'ms', 'collette', 'planning', 'her', 'new', 'life', 'should', 'be', 'chopped', 'off', 'and', 'sent', 'to', 'deleted', 'scenes', 'land', 'i', 'overkill', 'the', 'true', 'nature', 'of', 'her', 'physical', 'and', 'mental', 'ailments', 'should', 'be', 'obvious', 'by', 'the', 'time', 'mr', 'williams', 'returns', 'to', 'new', 'york', 'possibly', 'her', 'blindness', 'could', 'be', 'in', 'question', 'but', 'a', 'revelation', 'could', 'have', 'be', 'made', 'certain', 'in', 'either', 'the', 'highway', 'or', 'video', 'tape', 'scenes', 'the', 'film', 'would', 'benefit', 'from', 'a', 're', 'editing', 'how', 'about', 'a', 'directo', 'cut', 'williams', 'and', 'bobby', 'cannavale', 'as', 'jess', 'do', 'seem', 'initially', 'believable', 'as', 'a', 'couple', 'a', 'scene', 'or', 'two', 'establishing', 'their', 'relationship', 'might', 'have', 'helped', 'set', 'the', 'stage', 'otherwise', 'the', 'cast', 'is', 'exemplary', 'williams', 'offers', 'an', 'exceptionally', 'strong', 'characterization', 'and', 'not', 'a', 'gay', 'impersonation', 'sandra', 'oh', 'as', 'anna', 'joe', 'morton', 'as', 'ashe', 'and', 'rory', 'culkin', 'pete', 'logand', 'are', 'all', 'perfect', 'best', 'of', 'all', 'collett', 'donna', 'belongs', 'in', 'the', 'creepy', 'hall', 'of', 'fame', 'ms', 'oh', 'is', 'correct', 'in', 'saying', 'collette', 'might', 'be', 'you', 'know', 'like', 'that', 'guy', 'from', 'psycho', 'there', 'have', 'been', 'several', 'years', 'when', 'organizations', 'giving', 'acting', 'awards', 'seemed', 'to', 'reach', 'for', 'women', 'due', 'to', 'a', 'slighter', 'dispersion', 'of', 'roles', 'certainly', 'they', 'could', 'have', 'noticed', 'collette', 'with', 'some', 'award', 'consideration', 'she', 'is', 'that', 'good', 'and', 'director', 'patrick', 'stettner', 'definitely', 'evokes', 'hitchcock', 'he', 'even', 'makes', 'getting', 'a', 'sandwich', 'from', 'a', 'vending', 'machine', 'suspenseful', 'finally', 'writers', 'stettner', 'armistead', 'maupin', 'and', 'terry', 'anderson', 'deserve', 'gratitude', 'from', 'flight', 'attendants', 'everywhere', 'the', 'night', 'listener', '1', '21', '06', 'patrick', 'stettner', 'robin', 'williams', 'toni', 'collette', 'sandra', 'oh', 'rory', 'culkin'], tags=['train/pos/10006_7.txt']),\n",
       " TaggedDocument(words=['the', 'night', 'listener', '2006', '1', '2', 'robin', 'williams', 'toni', 'collette', 'bobby', 'cannavale', 'rory', 'culkin', 'joe', 'morton', 'sandra', 'oh', 'john', 'cullum', 'lisa', 'emery', 'becky', 'ann', 'baker', 'dir', 'patrick', 'stettner', 'hitchcockian', 'suspenser', 'gives', 'williams', 'a', 'stand', 'out', 'low', 'key', 'performance', 'what', 'is', 'it', 'about', 'celebrities', 'and', 'fans', 'what', 'is', 'the', 'near', 'paranoia', 'one', 'associates', 'with', 'the', 'other', 'and', 'why', 'is', 'it', 'almost', 'the', 'norm', 'in', 'the', 'latest', 'derange', 'fan', 'scenario', 'based', 'on', 'true', 'events', 'no', 'less', 'williams', 'stars', 'as', 'a', 'talk', 'radio', 'personality', 'named', 'gabriel', 'no', 'one', 'who', 'reads', 'stories', 'h', 'penned', 'over', 'the', 'airwaves', 'and', 'has', 'accumulated', 'an', 'interesting', 'fan', 'in', 'the', 'form', 'of', 'a', 'young', 'boy', 'named', 'pete', 'logand', 'culkin', 'who', 'has', 'submitted', 'a', 'manuscript', 'about', 'the', 'travails', 'of', 'his', 'troubled', 'youth', 'to', 'no', 'on', 'editor', 'ashe', 'morton', 'who', 'gives', 'it', 'to', 'no', 'one', 'to', 'read', 'for', 'himself', 'no', 'one', 'is', 'naturally', 'disturbed', 'but', 'ultimately', 'intrigued', 'about', 'the', 'nightmarish', 'existence', 'of', 'pete', 'being', 'abducted', 'and', 'sexually', 'abused', 'for', 'years', 'until', 'he', 'was', 'finally', 'rescued', 'by', 'a', 'nurse', 'named', 'donna', 'collette', 'giving', 'an', 'excellent', 'performance', 'who', 'has', 'adopted', 'the', 'boy', 'but', 'her', 'correspondence', 'with', 'no', 'one', 'reveals', 'that', 'pete', 'is', 'dying', 'from', 'aids', 'naturally', 'no', 'one', 'wants', 'to', 'meet', 'the', 'fans', 'but', 'is', 'suddenly', 'in', 'doubt', 'to', 'their', 'possibly', 'devious', 'ulterior', 'motives', 'when', 'the', 'seed', 'is', 'planted', 'by', 'his', 'estranged', 'lover', 'jess', 'cannavale', 'whose', 'sudden', 'departure', 'from', 'their', 'new', 'york', 'city', 'apartment', 'has', 'no', 'one', 'in', 'an', 'emotional', 'tailspin', 'that', 'has', 'only', 'now', 'grown', 'into', 'a', 'tempest', 'in', 'a', 'teacup', 'when', 'he', 'decides', 'to', 'do', 'some', 'investigating', 'into', 'donna', 'and', 'pet', 'backgrounds', 'discovering', 'some', 'truths', 'that', 'he', 'did', 'anticipate', 'written', 'by', 'armistead', 'maupin', 'who', 'co', 'wrote', 'the', 'screenplay', 'with', 'his', 'former', 'lover', 'terry', 'anderson', 'and', 'the', 'fil', 'novice', 'director', 'stettner', 'and', 'based', 'on', 'a', 'true', 'story', 'about', 'a', 'fa', 'hoax', 'found', 'out', 'has', 'some', 'hitchcockian', 'moments', 'that', 'run', 'on', 'full', 'tilt', 'like', 'any', 'good', 'old', 'fashioned', 'pot', 'boiler', 'does', 'it', 'helps', 'that', 'williams', 'gives', 'a', 'stand', 'out', 'low', 'key', 'performance', 'as', 'the', 'conflicted', 'good', 'hearted', 'personality', 'who', 'genuinely', 'wants', 'to', 'believe', 'that', 'his', 'number', 'one', 'fan', 'is', 'in', 'fact', 'real', 'and', 'does', 'love', 'him', 'the', 'one', 'thing', 'that', 'has', 'escaped', 'his', 'own', 'reality', 'and', 'has', 'some', 'unsettling', 'dreadful', 'moments', 'with', 'the', 'creepy', 'collette', 'whose', 'one', 'physical', 'trait', 'i', 'will', 'leave', 'unmentioned', 'but', 'underlines', 'the', 'desperation', 'of', 'her', 'character', 'that', 'can', 'rattle', 'you', 'to', 'the', 'core', 'however', 'the', 'film', 'runs', 'out', 'of', 'gas', 'and', 'eventually', 'becomes', 'a', 'bit', 'repetitive', 'and', 'predictable', 'despite', 'a', 'finely', 'directed', 'piece', 'of', 'hoodwink', 'and', 'mystery', 'by', 'stettner', 'it', 'pays', 'to', 'listen', 'to', 'your', 'own', 'inner', 'voice', 'be', 'careful', 'of', 'what', 'you', 'hope', 'for'], tags=['train/pos/10007_7.txt']),\n",
       " TaggedDocument(words=['you', 'know', 'robin', 'williams', 'god', 'bless', 'him', 'is', 'constantly', 'shooting', 'himself', 'in', 'the', 'foot', 'lately', 'with', 'all', 'these', 'dumb', 'comedies', 'he', 'has', 'done', 'this', 'decade', 'with', 'perhaps', 'the', 'exception', 'of', 'death', 'to', 'smoochy', 'which', 'bombed', 'when', 'it', 'came', 'out', 'but', 'is', 'now', 'a', 'cult', 'classic', 'the', 'dramas', 'he', 'has', 'made', 'lately', 'have', 'been', 'fantastic', 'especially', 'insomnia', 'and', 'one', 'hour', 'photo', 'the', 'night', 'listener', 'despite', 'mediocre', 'reviews', 'and', 'a', 'quick', 'dvd', 'release', 'is', 'among', 'his', 'best', 'work', 'period', 'this', 'is', 'a', 'very', 'chilling', 'story', 'even', 'though', 'it', 'does', 'include', 'a', 'serial', 'killer', 'or', 'anyone', 'that', 'physically', 'dangerous', 'for', 'that', 'matter', 'the', 'concept', 'of', 'the', 'film', 'is', 'based', 'on', 'an', 'actual', 'case', 'of', 'fraud', 'that', 'still', 'has', 'yet', 'to', 'be', 'officially', 'confirmed', 'in', 'high', 'school', 'i', 'read', 'an', 'autobiography', 'by', 'a', 'child', 'named', 'anthony', 'godby', 'johnson', 'who', 'suffered', 'horrific', 'abuse', 'and', 'eventually', 'contracted', 'aids', 'as', 'a', 'result', 'i', 'was', 'moved', 'by', 'the', 'story', 'until', 'i', 'read', 'reports', 'online', 'that', 'johnson', 'may', 'not', 'actually', 'exist', 'when', 'i', 'saw', 'this', 'movie', 'the', 'confused', 'feelings', 'that', 'robin', 'williams', 'so', 'brilliantly', 'portrayed', 'resurfaced', 'in', 'my', 'mind', 'toni', 'collette', 'probably', 'gives', 'her', 'best', 'dramatic', 'performance', 'too', 'as', 'the', 'ultimately', 'sociopathic', 'caretaker', 'her', 'role', 'was', 'a', 'far', 'cry', 'from', 'those', 'she', 'had', 'in', 'movies', 'like', 'little', 'miss', 'sunshine', 'there', 'were', 'even', 'times', 'she', 'looked', 'into', 'the', 'camera', 'where', 'i', 'thought', 'she', 'was', 'staring', 'right', 'at', 'me', 'it', 'takes', 'a', 'good', 'actress', 'to', 'play', 'that', 'sort', 'of', 'role', 'and', 'i', 'this', 'understated', 'yet', 'well', 'reviewed', 'role', 'that', 'makes', 'toni', 'collette', 'probably', 'one', 'of', 'the', 'best', 'actresses', 'of', 'this', 'generation', 'not', 'to', 'have', 'even', 'been', 'nominated', 'for', 'an', 'academy', 'award', 'as', 'of', '2008', 'i', 'incredible', 'that', 'there', 'is', 'at', 'least', 'one', 'woman', 'in', 'this', 'world', 'who', 'is', 'like', 'this', 'and', 'i', 'scary', 'too', 'this', 'is', 'a', 'good', 'dark', 'film', 'that', 'i', 'highly', 'recommend', 'be', 'prepared', 'to', 'be', 'unsettled', 'though', 'because', 'this', 'movie', 'leaves', 'you', 'with', 'a', 'strange', 'feeling', 'at', 'the', 'end'], tags=['train/pos/10008_7.txt'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsup_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a4f86",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04599347",
   "metadata": {},
   "source": [
    "a) Shuffle the documents using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dbd9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class PermuteSentences(object):\n",
    "    def __init__(self, sents):\n",
    "        self.sents = sents\n",
    "        \n",
    "    def __iter__(self):\n",
    "        shuffled = list(self.sents)\n",
    "        random.shuffle(shuffled)\n",
    "        for sent in shuffled:\n",
    "            yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5039ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 00:35:18,993 :INFO : collecting all words and their counts\n",
      "2022-10-30 00:35:19,123 :INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2022-10-30 00:35:19,396 :INFO : PROGRESS: at example #10000, processed 1436415 words (5269542/s), 45926 word types, 10000 tags\n",
      "2022-10-30 00:35:19,671 :INFO : PROGRESS: at example #20000, processed 2834721 words (5093790/s), 62055 word types, 20000 tags\n",
      "2022-10-30 00:35:20,515 :INFO : PROGRESS: at example #30000, processed 4246007 words (1673026/s), 73635 word types, 30000 tags\n",
      "2022-10-30 00:35:20,819 :INFO : PROGRESS: at example #40000, processed 5668828 words (4693729/s), 83213 word types, 40000 tags\n",
      "2022-10-30 00:35:21,106 :INFO : PROGRESS: at example #50000, processed 7093178 words (4983443/s), 91608 word types, 50000 tags\n",
      "2022-10-30 00:35:21,363 :INFO : PROGRESS: at example #60000, processed 8493648 words (5454155/s), 99051 word types, 60000 tags\n",
      "2022-10-30 00:35:21,644 :INFO : PROGRESS: at example #70000, processed 9884246 words (4956572/s), 105601 word types, 70000 tags\n",
      "2022-10-30 00:35:21,891 :INFO : PROGRESS: at example #80000, processed 11301565 words (5737099/s), 111599 word types, 80000 tags\n",
      "2022-10-30 00:35:22,135 :INFO : PROGRESS: at example #90000, processed 12702203 words (5755633/s), 117314 word types, 90000 tags\n",
      "2022-10-30 00:35:22,391 :INFO : PROGRESS: at example #100000, processed 14136223 words (5630713/s), 122811 word types, 100000 tags\n",
      "2022-10-30 00:35:22,633 :INFO : PROGRESS: at example #110000, processed 15548967 words (5840507/s), 127903 word types, 110000 tags\n",
      "2022-10-30 00:35:22,874 :INFO : PROGRESS: at example #120000, processed 16973413 words (5941447/s), 132676 word types, 120000 tags\n",
      "2022-10-30 00:35:23,158 :INFO : PROGRESS: at example #130000, processed 18374676 words (4942121/s), 137454 word types, 130000 tags\n",
      "2022-10-30 00:35:23,434 :INFO : PROGRESS: at example #140000, processed 19786715 words (5131589/s), 141717 word types, 140000 tags\n",
      "2022-10-30 00:35:23,675 :INFO : PROGRESS: at example #150000, processed 21214299 words (5928849/s), 145971 word types, 150000 tags\n",
      "2022-10-30 00:35:23,974 :INFO : PROGRESS: at example #160000, processed 22635958 words (4772233/s), 150004 word types, 160000 tags\n",
      "2022-10-30 00:35:24,280 :INFO : PROGRESS: at example #170000, processed 24022472 words (4523876/s), 153950 word types, 170000 tags\n",
      "2022-10-30 00:35:25,362 :INFO : collected 156031 word types and 175325 unique tags from a corpus of 175325 examples and 24777191 words\n",
      "2022-10-30 00:35:25,363 :INFO : Creating a fresh vocabulary\n",
      "2022-10-30 00:35:25,585 :INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 58035 unique words (37.194531855849156%% of original 156031, drops 97996)', 'datetime': '2022-10-30T00:35:25.585932', 'gensim': '4.0.1', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-30 00:35:25,585 :INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 24620467 word corpus (99.36746663493857%% of original 24777191, drops 156724)', 'datetime': '2022-10-30T00:35:25.585932', 'gensim': '4.0.1', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-30 00:35:25,891 :INFO : deleting the raw counts dictionary of 156031 items\n",
      "2022-10-30 00:35:25,896 :INFO : sample=0.001 downsamples 47 most-common words\n",
      "2022-10-30 00:35:25,897 :INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 18633524.774013627 word corpus (75.7%% of prior 24620467)', 'datetime': '2022-10-30T00:35:25.897654', 'gensim': '4.0.1', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-30 00:35:25,935 :INFO : constructing a huffman tree from 58035 words\n",
      "2022-10-30 00:35:28,026 :INFO : built huffman tree with maximum node depth 22\n",
      "2022-10-30 00:35:28,687 :INFO : estimated required memory for 58035 words and 50 dimensions: 145575500 bytes\n",
      "2022-10-30 00:35:28,688 :INFO : resetting layer weights\n",
      "2022-10-30 00:35:28,776 :INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 58035 vocabulary and 50 features, using sg=1 hs=1 sample=0.001 negative=5 window=5', 'datetime': '2022-10-30T00:35:28.776983', 'gensim': '4.0.1', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2022-10-30 00:35:29,789 :INFO : EPOCH 1 - PROGRESS: at 2.29% examples, 432716 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:35:30,790 :INFO : EPOCH 1 - PROGRESS: at 5.41% examples, 517493 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:35:31,794 :INFO : EPOCH 1 - PROGRESS: at 8.61% examples, 543123 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:32,796 :INFO : EPOCH 1 - PROGRESS: at 12.02% examples, 565860 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:33,796 :INFO : EPOCH 1 - PROGRESS: at 15.38% examples, 577032 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:34,811 :INFO : EPOCH 1 - PROGRESS: at 18.61% examples, 584802 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:35,819 :INFO : EPOCH 1 - PROGRESS: at 22.06% examples, 590088 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:36,823 :INFO : EPOCH 1 - PROGRESS: at 25.35% examples, 594240 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:37,834 :INFO : EPOCH 1 - PROGRESS: at 28.57% examples, 595342 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:38,836 :INFO : EPOCH 1 - PROGRESS: at 31.71% examples, 595495 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:39,850 :INFO : EPOCH 1 - PROGRESS: at 34.69% examples, 591661 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:40,873 :INFO : EPOCH 1 - PROGRESS: at 38.08% examples, 593618 words/s, in_qsize 4, out_qsize 1\n",
      "2022-10-30 00:35:41,880 :INFO : EPOCH 1 - PROGRESS: at 40.90% examples, 589089 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:35:42,880 :INFO : EPOCH 1 - PROGRESS: at 44.00% examples, 589051 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:35:43,891 :INFO : EPOCH 1 - PROGRESS: at 46.66% examples, 582387 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:44,899 :INFO : EPOCH 1 - PROGRESS: at 49.92% examples, 583112 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:45,919 :INFO : EPOCH 1 - PROGRESS: at 52.62% examples, 578059 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:46,928 :INFO : EPOCH 1 - PROGRESS: at 55.74% examples, 578037 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:47,932 :INFO : EPOCH 1 - PROGRESS: at 58.94% examples, 578537 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:48,936 :INFO : EPOCH 1 - PROGRESS: at 62.27% examples, 580339 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:49,936 :INFO : EPOCH 1 - PROGRESS: at 64.97% examples, 576806 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:50,940 :INFO : EPOCH 1 - PROGRESS: at 68.14% examples, 577682 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:51,953 :INFO : EPOCH 1 - PROGRESS: at 71.17% examples, 576872 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:52,970 :INFO : EPOCH 1 - PROGRESS: at 74.29% examples, 576664 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:53,992 :INFO : EPOCH 1 - PROGRESS: at 77.29% examples, 575757 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:55,013 :INFO : EPOCH 1 - PROGRESS: at 81.10% examples, 580843 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:35:56,024 :INFO : EPOCH 1 - PROGRESS: at 84.75% examples, 584729 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:57,036 :INFO : EPOCH 1 - PROGRESS: at 88.24% examples, 587031 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:58,055 :INFO : EPOCH 1 - PROGRESS: at 91.39% examples, 587454 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:35:59,063 :INFO : EPOCH 1 - PROGRESS: at 95.09% examples, 590144 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:00,071 :INFO : EPOCH 1 - PROGRESS: at 98.41% examples, 591382 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:00,519 :INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-30 00:36:00,527 :INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 00:36:00,531 :INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-30 00:36:00,532 :INFO : EPOCH - 1 : training on 24777191 raw words (18806595 effective words) took 31.7s, 592414 effective words/s\n",
      "2022-10-30 00:36:01,543 :INFO : EPOCH 2 - PROGRESS: at 2.40% examples, 458460 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:02,556 :INFO : EPOCH 2 - PROGRESS: at 5.61% examples, 528417 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:03,565 :INFO : EPOCH 2 - PROGRESS: at 8.67% examples, 542549 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:04,570 :INFO : EPOCH 2 - PROGRESS: at 12.04% examples, 566125 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:05,577 :INFO : EPOCH 2 - PROGRESS: at 15.42% examples, 580190 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:06,581 :INFO : EPOCH 2 - PROGRESS: at 18.70% examples, 587657 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:07,582 :INFO : EPOCH 2 - PROGRESS: at 22.20% examples, 598622 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:08,593 :INFO : EPOCH 2 - PROGRESS: at 25.45% examples, 598566 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:09,595 :INFO : EPOCH 2 - PROGRESS: at 28.59% examples, 598124 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:10,639 :INFO : EPOCH 2 - PROGRESS: at 31.43% examples, 587388 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:11,647 :INFO : EPOCH 2 - PROGRESS: at 34.71% examples, 590457 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:12,654 :INFO : EPOCH 2 - PROGRESS: at 37.62% examples, 588221 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:13,667 :INFO : EPOCH 2 - PROGRESS: at 40.69% examples, 587238 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:14,674 :INFO : EPOCH 2 - PROGRESS: at 44.39% examples, 595664 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:15,684 :INFO : EPOCH 2 - PROGRESS: at 48.15% examples, 602400 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:16,704 :INFO : EPOCH 2 - PROGRESS: at 52.01% examples, 608319 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:17,706 :INFO : EPOCH 2 - PROGRESS: at 55.50% examples, 611154 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:18,737 :INFO : EPOCH 2 - PROGRESS: at 58.54% examples, 607729 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:19,738 :INFO : EPOCH 2 - PROGRESS: at 61.82% examples, 607847 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:20,756 :INFO : EPOCH 2 - PROGRESS: at 65.55% examples, 610966 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:21,759 :INFO : EPOCH 2 - PROGRESS: at 68.95% examples, 612696 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:22,760 :INFO : EPOCH 2 - PROGRESS: at 72.70% examples, 616228 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:23,761 :INFO : EPOCH 2 - PROGRESS: at 75.56% examples, 612808 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:24,763 :INFO : EPOCH 2 - PROGRESS: at 78.55% examples, 610915 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:25,776 :INFO : EPOCH 2 - PROGRESS: at 81.40% examples, 607081 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:26,778 :INFO : EPOCH 2 - PROGRESS: at 84.51% examples, 606164 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:27,787 :INFO : EPOCH 2 - PROGRESS: at 88.21% examples, 609525 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:28,809 :INFO : EPOCH 2 - PROGRESS: at 91.35% examples, 608923 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:29,835 :INFO : EPOCH 2 - PROGRESS: at 94.51% examples, 607454 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:30,847 :INFO : EPOCH 2 - PROGRESS: at 97.91% examples, 607789 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:31,383 :INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-30 00:36:31,394 :INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-30 00:36:31,403 :INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-30 00:36:31,403 :INFO : EPOCH - 2 : training on 24777191 raw words (18811280 effective words) took 30.9s, 609499 effective words/s\n",
      "2022-10-30 00:36:32,410 :INFO : EPOCH 3 - PROGRESS: at 3.23% examples, 588848 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:33,418 :INFO : EPOCH 3 - PROGRESS: at 6.57% examples, 605453 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:36:34,418 :INFO : EPOCH 3 - PROGRESS: at 10.09% examples, 621975 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:35,424 :INFO : EPOCH 3 - PROGRESS: at 13.66% examples, 632685 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:36,435 :INFO : EPOCH 3 - PROGRESS: at 17.15% examples, 637254 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:37,445 :INFO : EPOCH 3 - PROGRESS: at 20.76% examples, 643458 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:38,445 :INFO : EPOCH 3 - PROGRESS: at 24.04% examples, 641412 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:39,449 :INFO : EPOCH 3 - PROGRESS: at 27.19% examples, 634645 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:40,452 :INFO : EPOCH 3 - PROGRESS: at 30.46% examples, 633489 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:41,454 :INFO : EPOCH 3 - PROGRESS: at 33.70% examples, 629867 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:42,460 :INFO : EPOCH 3 - PROGRESS: at 37.47% examples, 637437 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:43,461 :INFO : EPOCH 3 - PROGRESS: at 41.24% examples, 642966 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:44,465 :INFO : EPOCH 3 - PROGRESS: at 44.96% examples, 647829 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:45,469 :INFO : EPOCH 3 - PROGRESS: at 48.45% examples, 648255 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:46,470 :INFO : EPOCH 3 - PROGRESS: at 51.93% examples, 648049 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:47,479 :INFO : EPOCH 3 - PROGRESS: at 55.34% examples, 647286 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:48,486 :INFO : EPOCH 3 - PROGRESS: at 58.74% examples, 646273 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:49,492 :INFO : EPOCH 3 - PROGRESS: at 61.77% examples, 642136 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:50,498 :INFO : EPOCH 3 - PROGRESS: at 64.97% examples, 640373 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:51,499 :INFO : EPOCH 3 - PROGRESS: at 67.80% examples, 635187 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:52,514 :INFO : EPOCH 3 - PROGRESS: at 70.85% examples, 631628 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:36:53,516 :INFO : EPOCH 3 - PROGRESS: at 73.81% examples, 628354 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:54,528 :INFO : EPOCH 3 - PROGRESS: at 76.95% examples, 625992 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:55,533 :INFO : EPOCH 3 - PROGRESS: at 80.48% examples, 626809 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:56,539 :INFO : EPOCH 3 - PROGRESS: at 83.83% examples, 626927 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:57,541 :INFO : EPOCH 3 - PROGRESS: at 87.50% examples, 629455 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:58,560 :INFO : EPOCH 3 - PROGRESS: at 91.08% examples, 631425 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:36:59,566 :INFO : EPOCH 3 - PROGRESS: at 94.85% examples, 633558 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:37:00,573 :INFO : EPOCH 3 - PROGRESS: at 98.56% examples, 635496 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:00,973 :INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-30 00:37:00,975 :INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-30 00:37:00,995 :INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-30 00:37:00,996 :INFO : EPOCH - 3 : training on 24777191 raw words (18807585 effective words) took 29.6s, 635689 effective words/s\n",
      "2022-10-30 00:37:02,011 :INFO : EPOCH 4 - PROGRESS: at 2.97% examples, 540506 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:03,012 :INFO : EPOCH 4 - PROGRESS: at 7.05% examples, 653322 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:04,012 :INFO : EPOCH 4 - PROGRESS: at 11.88% examples, 740408 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:37:05,019 :INFO : EPOCH 4 - PROGRESS: at 16.70% examples, 785082 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:37:06,021 :INFO : EPOCH 4 - PROGRESS: at 21.33% examples, 798235 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 00:37:07,033 :INFO : EPOCH 4 - PROGRESS: at 26.30% examples, 819017 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:08,038 :INFO : EPOCH 4 - PROGRESS: at 31.07% examples, 831365 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:09,043 :INFO : EPOCH 4 - PROGRESS: at 35.98% examples, 842626 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:10,052 :INFO : EPOCH 4 - PROGRESS: at 40.85% examples, 851776 words/s, in_qsize 4, out_qsize 1\n",
      "2022-10-30 00:37:11,053 :INFO : EPOCH 4 - PROGRESS: at 45.87% examples, 861635 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:12,063 :INFO : EPOCH 4 - PROGRESS: at 50.92% examples, 868221 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:13,065 :INFO : EPOCH 4 - PROGRESS: at 55.75% examples, 872721 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:37:14,065 :INFO : EPOCH 4 - PROGRESS: at 60.63% examples, 876155 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:15,070 :INFO : EPOCH 4 - PROGRESS: at 65.62% examples, 880001 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:37:16,086 :INFO : EPOCH 4 - PROGRESS: at 70.68% examples, 883159 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:17,091 :INFO : EPOCH 4 - PROGRESS: at 75.64% examples, 886397 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:18,093 :INFO : EPOCH 4 - PROGRESS: at 80.55% examples, 888923 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:37:19,094 :INFO : EPOCH 4 - PROGRESS: at 85.59% examples, 891684 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:37:20,109 :INFO : EPOCH 4 - PROGRESS: at 90.40% examples, 891678 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:21,113 :INFO : EPOCH 4 - PROGRESS: at 95.30% examples, 891989 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:37:22,098 :INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-30 00:37:22,106 :INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-30 00:37:22,110 :INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-30 00:37:22,111 :INFO : EPOCH - 4 : training on 24777191 raw words (18806194 effective words) took 21.1s, 891006 effective words/s\n",
      "2022-10-30 00:37:23,119 :INFO : EPOCH 5 - PROGRESS: at 4.23% examples, 779594 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:24,126 :INFO : EPOCH 5 - PROGRESS: at 9.18% examples, 845292 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:25,128 :INFO : EPOCH 5 - PROGRESS: at 14.15% examples, 875960 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:26,128 :INFO : EPOCH 5 - PROGRESS: at 18.99% examples, 886247 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:27,130 :INFO : EPOCH 5 - PROGRESS: at 23.51% examples, 878988 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:28,138 :INFO : EPOCH 5 - PROGRESS: at 28.45% examples, 890062 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:37:29,146 :INFO : EPOCH 5 - PROGRESS: at 33.44% examples, 896362 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:30,153 :INFO : EPOCH 5 - PROGRESS: at 38.39% examples, 900322 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:31,160 :INFO : EPOCH 5 - PROGRESS: at 43.36% examples, 905234 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:32,162 :INFO : EPOCH 5 - PROGRESS: at 48.43% examples, 908139 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:33,165 :INFO : EPOCH 5 - PROGRESS: at 53.31% examples, 909212 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:34,178 :INFO : EPOCH 5 - PROGRESS: at 58.31% examples, 911273 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:35,186 :INFO : EPOCH 5 - PROGRESS: at 63.18% examples, 910766 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:36,191 :INFO : EPOCH 5 - PROGRESS: at 68.16% examples, 912560 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:37,195 :INFO : EPOCH 5 - PROGRESS: at 72.93% examples, 911566 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:38,205 :INFO : EPOCH 5 - PROGRESS: at 77.79% examples, 910471 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:39,213 :INFO : EPOCH 5 - PROGRESS: at 82.86% examples, 911734 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:40,215 :INFO : EPOCH 5 - PROGRESS: at 87.76% examples, 912274 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:41,221 :INFO : EPOCH 5 - PROGRESS: at 92.57% examples, 912274 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:42,222 :INFO : EPOCH 5 - PROGRESS: at 97.71% examples, 914028 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:42,675 :INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-30 00:37:42,682 :INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-30 00:37:42,687 :INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-30 00:37:42,687 :INFO : EPOCH - 5 : training on 24777191 raw words (18806679 effective words) took 20.6s, 914259 effective words/s\n",
      "2022-10-30 00:37:43,704 :INFO : EPOCH 6 - PROGRESS: at 4.29% examples, 800399 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:44,706 :INFO : EPOCH 6 - PROGRESS: at 9.39% examples, 879435 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:45,717 :INFO : EPOCH 6 - PROGRESS: at 14.06% examples, 873761 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:46,724 :INFO : EPOCH 6 - PROGRESS: at 18.90% examples, 886321 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:37:47,727 :INFO : EPOCH 6 - PROGRESS: at 23.84% examples, 892878 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:48,743 :INFO : EPOCH 6 - PROGRESS: at 28.72% examples, 894482 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:49,765 :INFO : EPOCH 6 - PROGRESS: at 33.39% examples, 892036 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:50,774 :INFO : EPOCH 6 - PROGRESS: at 37.91% examples, 886239 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:51,797 :INFO : EPOCH 6 - PROGRESS: at 42.62% examples, 884024 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:52,802 :INFO : EPOCH 6 - PROGRESS: at 46.86% examples, 875641 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:53,815 :INFO : EPOCH 6 - PROGRESS: at 51.17% examples, 869743 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:54,815 :INFO : EPOCH 6 - PROGRESS: at 55.61% examples, 865808 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:55,816 :INFO : EPOCH 6 - PROGRESS: at 60.24% examples, 865104 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:56,822 :INFO : EPOCH 6 - PROGRESS: at 64.92% examples, 866798 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:57,824 :INFO : EPOCH 6 - PROGRESS: at 69.90% examples, 871089 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:58,831 :INFO : EPOCH 6 - PROGRESS: at 74.81% examples, 873101 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:37:59,831 :INFO : EPOCH 6 - PROGRESS: at 79.56% examples, 873831 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:00,835 :INFO : EPOCH 6 - PROGRESS: at 84.35% examples, 874949 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:01,841 :INFO : EPOCH 6 - PROGRESS: at 89.21% examples, 876141 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:02,849 :INFO : EPOCH 6 - PROGRESS: at 93.97% examples, 876652 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:03,863 :INFO : EPOCH 6 - PROGRESS: at 98.47% examples, 874833 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:04,185 :INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-30 00:38:04,188 :INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-30 00:38:04,205 :INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-30 00:38:04,205 :INFO : EPOCH - 6 : training on 24777191 raw words (18808040 effective words) took 21.5s, 874346 effective words/s\n",
      "2022-10-30 00:38:05,213 :INFO : EPOCH 7 - PROGRESS: at 4.11% examples, 776754 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:06,225 :INFO : EPOCH 7 - PROGRESS: at 9.05% examples, 850232 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:07,226 :INFO : EPOCH 7 - PROGRESS: at 14.14% examples, 880770 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:38:08,232 :INFO : EPOCH 7 - PROGRESS: at 18.57% examples, 873076 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:09,233 :INFO : EPOCH 7 - PROGRESS: at 22.52% examples, 845904 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:10,236 :INFO : EPOCH 7 - PROGRESS: at 27.19% examples, 847465 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:11,236 :INFO : EPOCH 7 - PROGRESS: at 31.95% examples, 852167 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 00:38:12,246 :INFO : EPOCH 7 - PROGRESS: at 36.64% examples, 853760 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:13,251 :INFO : EPOCH 7 - PROGRESS: at 41.54% examples, 861982 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:14,259 :INFO : EPOCH 7 - PROGRESS: at 45.85% examples, 856466 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:15,259 :INFO : EPOCH 7 - PROGRESS: at 50.85% examples, 862814 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:16,267 :INFO : EPOCH 7 - PROGRESS: at 55.70% examples, 866739 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:17,271 :INFO : EPOCH 7 - PROGRESS: at 60.79% examples, 873812 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:18,274 :INFO : EPOCH 7 - PROGRESS: at 65.57% examples, 876355 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:19,275 :INFO : EPOCH 7 - PROGRESS: at 70.11% examples, 875053 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:20,280 :INFO : EPOCH 7 - PROGRESS: at 75.01% examples, 877802 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:21,292 :INFO : EPOCH 7 - PROGRESS: at 79.82% examples, 878969 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:38:22,300 :INFO : EPOCH 7 - PROGRESS: at 84.73% examples, 880749 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:23,303 :INFO : EPOCH 7 - PROGRESS: at 89.21% examples, 879494 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:24,308 :INFO : EPOCH 7 - PROGRESS: at 94.24% examples, 882687 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:25,310 :INFO : EPOCH 7 - PROGRESS: at 99.05% examples, 883110 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:25,496 :INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-30 00:38:25,506 :INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-30 00:38:25,507 :INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-30 00:38:25,507 :INFO : EPOCH - 7 : training on 24777191 raw words (18806495 effective words) took 21.3s, 883199 effective words/s\n",
      "2022-10-30 00:38:26,526 :INFO : EPOCH 8 - PROGRESS: at 4.04% examples, 774720 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:27,532 :INFO : EPOCH 8 - PROGRESS: at 8.57% examples, 818334 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:28,549 :INFO : EPOCH 8 - PROGRESS: at 13.09% examples, 826654 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:29,552 :INFO : EPOCH 8 - PROGRESS: at 17.60% examples, 831289 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:30,558 :INFO : EPOCH 8 - PROGRESS: at 22.04% examples, 828693 words/s, in_qsize 4, out_qsize 1\n",
      "2022-10-30 00:38:31,558 :INFO : EPOCH 8 - PROGRESS: at 27.02% examples, 847060 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:38:32,564 :INFO : EPOCH 8 - PROGRESS: at 32.16% examples, 859835 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:33,567 :INFO : EPOCH 8 - PROGRESS: at 37.26% examples, 869698 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:34,574 :INFO : EPOCH 8 - PROGRESS: at 42.32% examples, 876234 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:35,582 :INFO : EPOCH 8 - PROGRESS: at 47.27% examples, 882223 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:38:36,587 :INFO : EPOCH 8 - PROGRESS: at 51.97% examples, 881581 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:37,587 :INFO : EPOCH 8 - PROGRESS: at 55.73% examples, 869029 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:38,591 :INFO : EPOCH 8 - PROGRESS: at 60.27% examples, 868875 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:39,597 :INFO : EPOCH 8 - PROGRESS: at 65.27% examples, 872389 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:40,606 :INFO : EPOCH 8 - PROGRESS: at 70.21% examples, 875550 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:41,610 :INFO : EPOCH 8 - PROGRESS: at 74.86% examples, 874778 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:42,618 :INFO : EPOCH 8 - PROGRESS: at 79.72% examples, 876784 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:38:43,621 :INFO : EPOCH 8 - PROGRESS: at 84.53% examples, 878602 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:44,625 :INFO : EPOCH 8 - PROGRESS: at 89.29% examples, 879191 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:45,629 :INFO : EPOCH 8 - PROGRESS: at 93.83% examples, 877587 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:46,630 :INFO : EPOCH 8 - PROGRESS: at 98.56% examples, 878349 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:38:46,923 :INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-30 00:38:46,925 :INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-30 00:38:46,933 :INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-30 00:38:46,933 :INFO : EPOCH - 8 : training on 24777191 raw words (18809111 effective words) took 21.4s, 878572 effective words/s\n",
      "2022-10-30 00:38:47,950 :INFO : EPOCH 9 - PROGRESS: at 4.03% examples, 738739 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:48,950 :INFO : EPOCH 9 - PROGRESS: at 8.55% examples, 794389 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:49,958 :INFO : EPOCH 9 - PROGRESS: at 13.26% examples, 823676 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:38:50,960 :INFO : EPOCH 9 - PROGRESS: at 17.88% examples, 833135 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:51,974 :INFO : EPOCH 9 - PROGRESS: at 22.51% examples, 841911 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:52,981 :INFO : EPOCH 9 - PROGRESS: at 26.57% examples, 826361 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:53,985 :INFO : EPOCH 9 - PROGRESS: at 31.40% examples, 836848 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:54,999 :INFO : EPOCH 9 - PROGRESS: at 36.40% examples, 846141 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:38:56,008 :INFO : EPOCH 9 - PROGRESS: at 41.23% examples, 852463 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:57,024 :INFO : EPOCH 9 - PROGRESS: at 45.99% examples, 853372 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:58,040 :INFO : EPOCH 9 - PROGRESS: at 50.46% examples, 851436 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:38:59,047 :INFO : EPOCH 9 - PROGRESS: at 55.02% examples, 851525 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:00,064 :INFO : EPOCH 9 - PROGRESS: at 59.93% examples, 858464 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:39:01,068 :INFO : EPOCH 9 - PROGRESS: at 64.86% examples, 864044 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:02,074 :INFO : EPOCH 9 - PROGRESS: at 69.99% examples, 869217 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:03,082 :INFO : EPOCH 9 - PROGRESS: at 75.20% examples, 874625 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:04,087 :INFO : EPOCH 9 - PROGRESS: at 80.23% examples, 878620 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:39:05,087 :INFO : EPOCH 9 - PROGRESS: at 85.32% examples, 882335 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:06,095 :INFO : EPOCH 9 - PROGRESS: at 90.27% examples, 885037 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:07,096 :INFO : EPOCH 9 - PROGRESS: at 95.20% examples, 887912 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:08,048 :INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-30 00:39:08,056 :INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-30 00:39:08,068 :INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-30 00:39:08,069 :INFO : EPOCH - 9 : training on 24777191 raw words (18808899 effective words) took 21.1s, 890196 effective words/s\n",
      "2022-10-30 00:39:09,082 :INFO : EPOCH 10 - PROGRESS: at 4.32% examples, 817439 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:10,082 :INFO : EPOCH 10 - PROGRESS: at 9.05% examples, 860493 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:11,089 :INFO : EPOCH 10 - PROGRESS: at 14.21% examples, 889903 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:39:12,089 :INFO : EPOCH 10 - PROGRESS: at 19.35% examples, 906332 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:13,091 :INFO : EPOCH 10 - PROGRESS: at 24.28% examples, 912420 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:14,098 :INFO : EPOCH 10 - PROGRESS: at 29.21% examples, 912555 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:15,106 :INFO : EPOCH 10 - PROGRESS: at 34.25% examples, 917351 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:16,110 :INFO : EPOCH 10 - PROGRESS: at 39.31% examples, 921589 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 00:39:17,110 :INFO : EPOCH 10 - PROGRESS: at 44.27% examples, 924309 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:18,117 :INFO : EPOCH 10 - PROGRESS: at 49.41% examples, 928973 words/s, in_qsize 6, out_qsize 0\n",
      "2022-10-30 00:39:19,124 :INFO : EPOCH 10 - PROGRESS: at 54.51% examples, 930206 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:20,131 :INFO : EPOCH 10 - PROGRESS: at 59.39% examples, 928076 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:21,139 :INFO : EPOCH 10 - PROGRESS: at 64.34% examples, 928318 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:22,157 :INFO : EPOCH 10 - PROGRESS: at 69.43% examples, 929480 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:23,169 :INFO : EPOCH 10 - PROGRESS: at 74.54% examples, 930385 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:24,172 :INFO : EPOCH 10 - PROGRESS: at 79.66% examples, 931623 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:25,179 :INFO : EPOCH 10 - PROGRESS: at 84.51% examples, 929458 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:26,188 :INFO : EPOCH 10 - PROGRESS: at 89.46% examples, 929637 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:27,192 :INFO : EPOCH 10 - PROGRESS: at 94.34% examples, 928746 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:28,200 :INFO : EPOCH 10 - PROGRESS: at 99.46% examples, 929423 words/s, in_qsize 5, out_qsize 0\n",
      "2022-10-30 00:39:28,316 :INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-30 00:39:28,326 :INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-30 00:39:28,327 :INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-30 00:39:28,327 :INFO : EPOCH - 10 : training on 24777191 raw words (18809090 effective words) took 20.3s, 928762 effective words/s\n",
      "2022-10-30 00:39:28,328 :INFO : Doc2Vec lifecycle event {'msg': 'training on 247771910 raw words (188079968 effective words) took 239.6s, 785139 effective words/s', 'datetime': '2022-10-30T00:39:28.328660', 'gensim': '4.0.1', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2022-10-30 00:39:28,328 :INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec(dbow,d50,n5,hs,mc5,s0.001,t3)', 'datetime': '2022-10-30T00:39:28.328660', 'gensim': '4.0.1', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "permuter = PermuteSentences(unsup_sentences)\n",
    "model = Doc2Vec(permuter,dm=0,hs=1, vector_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3835e",
   "metadata": {},
   "source": [
    "b) Using the scikit learn docs, explain the function of the code in part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7150a707",
   "metadata": {},
   "source": [
    "Firstly, we create a class called PermuteSentences. The class allows us to pass in the object and turn it into a list, then we use the shuffle method from random to shuffle the list. We pass in the unsup_sentences (which stores all the reviews) as an object to the PermuteSentences and store them in a variable called permuter. From the scikit learn docs of Doc2Vec, the first parameter we need is the documents. Documents is a list of TaggedDocument that we want to pass in for training. In our case, is the shuffle unsup_sentences. The second parameter dm defines the training algorithm that we want to specify for this training. It allows 1 or 0 as input, where 1 indicates 'distributed memory' and 0 is distributed bag of words. The third parameter is hs, which is whether to use hierarchical softmax for training. It allows 0 or 1 as input, where 1 indicates hierarchical softmax will be used and 0 indicates negative sampling will be used. The last parameter size is equal to 50, which means the vector representing each document will contain 50 elements. The vector maps the document to a point in 50-dimensional space. Higher dimensions allow more differentiation between documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3066f1a",
   "metadata": {},
   "source": [
    "c) Once the training is complete delete all but the inference data with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "028e176b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Doc2Vec' object has no attribute 'delete_temporary_training_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7b1674fe6273>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# done with training, free up some memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete_temporary_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep_inference\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Doc2Vec' object has no attribute 'delete_temporary_training_data'"
     ]
    }
   ],
   "source": [
    "# done with training, free up some memory\n",
    "model.delete_temporary_training_data(keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ed56b",
   "metadata": {},
   "source": [
    "d) At this point this is a solid model that can be used in a product. Save the model so that it can be loaded later with model = DocsVec.Load(‘reviews.d2v’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53399f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 11:07:18,490 :INFO : Doc2Vec lifecycle event {'fname_or_handle': 'reviews.d2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-10-30T11:07:18.490616', 'gensim': '4.0.1', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "2022-10-30 11:07:18,505 :INFO : not storing attribute cum_table\n",
      "2022-10-30 11:07:20,311 :INFO : saved reviews.d2v\n"
     ]
    }
   ],
   "source": [
    "model.save('reviews.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b891d",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59f393",
   "metadata": {},
   "source": [
    "a) Use model.infer, and the utility function to infer the vector for the phrase “This place is not worth your time, let alone Vegas.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bf5c370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24580933,  0.00514891,  0.3595308 , -0.52849627,  0.36814606,\n",
       "        0.38301033,  0.3920034 ,  0.03434087, -0.03278954, -0.2591965 ,\n",
       "        0.2423288 ,  0.07339396, -0.00290345, -0.26203755, -0.13965148,\n",
       "        0.02828451, -0.18127406, -0.2573375 , -0.19473793,  0.00873207,\n",
       "       -0.10308146,  0.20605232,  0.07073209, -0.63871187, -0.37917265,\n",
       "       -0.08233731,  0.21018082,  0.09837718,  0.6126511 , -0.03327044,\n",
       "        0.6493887 , -0.17822012, -0.04948199,  0.06823016, -0.10996078,\n",
       "        0.08465364,  0.20656943, -0.2342572 ,  0.16749963,  0.04028142,\n",
       "       -0.04747786,  0.5186926 , -0.07909732, -0.16011365, -0.35905966,\n",
       "        0.52198356, -0.37370095, -0.06028552, -0.3244094 , -0.1772991 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(extract_words(\"This place is not worth your time, let alone Vegas.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbfbdab",
   "metadata": {},
   "source": [
    "b) Import cosine_similarity. Refer to the code snippet that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73ceb817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9431acc",
   "metadata": {},
   "source": [
    "c) Use cosine similarity to compare the following comments. <br>\n",
    "> I. “this place is not worth your time, let alone Vegas.”<br>\n",
    "   II. “service sucks.”<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5830b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5614059]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "    [model.infer_vector(extract_words(\"this place is not worth your time, let alone Vegas.\"))],\n",
    "    [model.infer_vector(extract_words(\"service sucks.\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ecd16",
   "metadata": {},
   "source": [
    "d) Use cosine similarity to compare the following comments.<br>\n",
    ">I. “highly recommended.”<br>\n",
    "II. “service sucks.”<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cac47da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30124742]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "    [model.infer_vector(extract_words(\"highly recommended.\"))],\n",
    "    [model.infer_vector(extract_words(\"service sucks.\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb74634",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf625e",
   "metadata": {},
   "source": [
    "a) Use the following code to ready the data you will use. The Yelp, Amazon, and imdb comments are available online again at the UCI ML Repository page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d59bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentvecs = []\n",
    "sentiments = []\n",
    "for fname in [\"yelp\", \"amazon_cells\", \"imdb\"]: \n",
    "    with open(\"sentiment labelled sentences/%s_labelled.txt\" % fname, encoding='UTF-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line_split = line.strip().split('\\t')\n",
    "            sentences.append(line_split[0])\n",
    "            words = extract_words(line_split[0])\n",
    "            sentvecs.append(model.infer_vector(words, steps=10)) # create a vector for this document\n",
    "            sentiments.append(int(line_split[1]))\n",
    "            \n",
    "# shuffle sentences, sentvecs, sentiments together\n",
    "combined = list(zip(sentences, sentvecs, sentiments))\n",
    "random.shuffle(combined)\n",
    "sentences, sentvecs, sentiments = zip(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16ec16",
   "metadata": {},
   "source": [
    "b) Explain what the code in part (a) has done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d76b2",
   "metadata": {},
   "source": [
    "The code in part (a) first creates three empty lists (sentences, sentvecs, sentiments). The for loop opens all the txt files in the sentiment labelled sentences folder. For each line of the txt file, it includes a review and a sentiment (0 for negative and 1 for positive). The first line of the code inside the for loop split the review and the sentiment. The review will store in the sentences list. Then, we use the infer_vector method to create a vector for the review. The sentiment for each review will store in the sentiments list. After the for loop, we use the zip() function to merge and match the three lists (sentences, sentvecs, sentiments). Then we turn the combined list from a tuple to a list. After that, the shuffle method from random will be used the shuffle the combined list. In the end, we use zip() along with the unpacking operator * to separate the elements of each tuple into independent sequences (sentences, sentvecs, sentiments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35deebc6",
   "metadata": {},
   "source": [
    "c) Import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "682a85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77d1fa",
   "metadata": {},
   "source": [
    "d) Create an instance of KNeighborsClassifier with 9 neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a45bbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e8f0e",
   "metadata": {},
   "source": [
    "e) Output the cross validated scores for the KNC. Use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dd478bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7883333333333333, 0.015129074290546971)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "scores = cross_val_score(clf, sentvecs, sentiments, cv=5)\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f76a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
